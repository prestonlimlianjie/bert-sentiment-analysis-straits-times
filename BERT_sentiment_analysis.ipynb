{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD4SoOryI4Hu",
        "colab_type": "text"
      },
      "source": [
        "## **Classifying the sentiments of Straits Times Facebook comments**\n",
        "\n",
        "Given a user's comment on articles posted by the Straits Times on Facebook, how can we identify what the user feels? This project assumes that there are 3 possible sentiments: 'positive', 'negative', and 'neutral'.\n",
        "\n",
        "For this model, we will fine-tune BERT to perform the sentiment classification task. \n",
        "\n",
        "Most of the work here is based on the repo: https://github.com/sebsk/CS224N-Project\n",
        "\n",
        "### **Classifier model**\n",
        "\n",
        "Since we are using `BertForSequenceClassification`, the model will take in the input and mask tensors and produce an single tensor of size 1 x 768. This tensor is the BERT output of the `[CLS]` token.\n",
        "\n",
        "The `BertForSequenceClassification` model will then output this tensor to a softmax layer of size `n_class` (which in our case is 3 since we have 3 possible classes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXLCJVyTx0FS",
        "colab_type": "code",
        "outputId": "34f698d3-9857-4ee3-d364-9a2fea53fdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torch transformers pandas scikit-learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbzCqAbUx02u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define utils functions\n",
        "\n",
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "    @param sents (list[list[int]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (int): padding token\n",
        "    @returns sents_padded (list[list[int]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "        Output shape: (batch_size, max_sentence_length)\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    max_len = max(len(s) for s in sents)\n",
        "    batch_size = len(sents)\n",
        "\n",
        "    for s in sents:\n",
        "        padded = [pad_token] * max_len\n",
        "        padded[:len(s)] = s\n",
        "        sents_padded.append(padded)\n",
        "\n",
        "    return sents_padded\n",
        "\n",
        "def sents_to_tensor(tokenizer, sents, device):\n",
        "    \"\"\"\n",
        "    :param tokenizer: BertTokenizer\n",
        "    :param sents: list[str], list of sentences (NOTE: untokenized, continuous sentences), reversely sorted\n",
        "    :param device: torch.device\n",
        "    :return: sents_tensor: torch.Tensor, shape(batch_size, max_sent_length), reversely sorted\n",
        "    :return: masks_tensor: torch.Tensor, shape(batch_size, max_sent_length), reversely sorted\n",
        "    :return: sents_lengths: torch.Tensor, shape(batch_size), reversely sorted\n",
        "    \"\"\"\n",
        "    tokens_list = [tokenizer.tokenize(sent) for sent in sents]\n",
        "    sents_lengths = [len(tokens) for tokens in tokens_list]\n",
        "    # tokens_sents_zip = zip(tokens_list, sents_lengths)\n",
        "    # tokens_sents_zip = sorted(tokens_sents_zip, key=lambda x: x[1], reverse=True)\n",
        "    # tokens_list, sents_lengths = zip(*tokens_sents_zip)\n",
        "    tokens_list_padded = pad_sents(tokens_list, '[PAD]')\n",
        "    sents_lengths = torch.tensor(sents_lengths, device=device)\n",
        "\n",
        "    masks = []\n",
        "    for tokens in tokens_list_padded:\n",
        "        mask = [0 if token=='[PAD]' else 1 for token in tokens]\n",
        "        masks.append(mask)\n",
        "    masks_tensor = torch.tensor(masks, dtype=torch.long, device=device)\n",
        "    tokens_id_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_list_padded]\n",
        "    sents_tensor = torch.tensor(tokens_id_list, dtype=torch.long, device=device)\n",
        "\n",
        "    return sents_tensor, masks_tensor, sents_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6UKNnv2I8-C",
        "colab_type": "text"
      },
      "source": [
        "## **Defining the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMZnaVNx5Gu",
        "colab_type": "code",
        "outputId": "2585089c-7dca-47a1-a209-89b071e172b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOVH2otOyOBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the sentiment classification model\n",
        "\n",
        "class SentimentClassifierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_config, device, n_class):\n",
        "        \"\"\"\n",
        "        :param bert_config: str, BERT configuration description\n",
        "        :param device: torch.device\n",
        "        :param n_class: int\n",
        "        \"\"\"\n",
        "\n",
        "        super(SentimentClassifierModel, self).__init__()\n",
        "\n",
        "        self.n_class = n_class\n",
        "        self.bert_config = bert_config\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(self.bert_config, num_labels=self.n_class)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_config)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, sents):\n",
        "        \"\"\"\n",
        "        :param sents: list[str], list of sentences (NOTE: untokenized, continuous sentences)\n",
        "        :return: pre_softmax, torch.tensor of shape (batch_size, n_class)\n",
        "        \"\"\"\n",
        "\n",
        "        sents_tensor, masks_tensor, sents_lengths = sents_to_tensor(self.tokenizer, sents, self.device)\n",
        "        pre_softmax = self.bert(input_ids=sents_tensor, attention_mask=masks_tensor)\n",
        "\n",
        "        return pre_softmax\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str, device):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        @return model (nn.Module): model with saved parameters\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = SentimentClassifierModel(device=device, **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the model to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(bert_config=self.bert_config, n_class=self.n_class),\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DKzAYcHylDM",
        "colab_type": "text"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "We use the US Twitter Airline Sentiment dataset for training: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
        "\n",
        "The CSV file has the following rows: \n",
        "- `tweet_id`\n",
        "- `airline_sentiment`\n",
        "- `airline_sentiment_confidence`\n",
        "- `negativereason`\n",
        "- `negativereason_confidence`\n",
        "- `airline`\n",
        "- `airline_sentiment_gold`\n",
        "- `name`\n",
        "- `negativereason_gold`\n",
        "- `retweet_count`\n",
        "- `text`\n",
        "- `tweet_coord`\n",
        "- `tweet_created`\n",
        "- `tweet_location`\n",
        "- `user_timezone`\n",
        "\n",
        "For our purposes, we only care about the following rows:\n",
        "- `tweet_id`\n",
        "- `airline_sentiment`\n",
        "- `text`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js_wLKomytuz",
        "colab_type": "code",
        "outputId": "cf17d1e8-0df7-4dce-caae-42d77f8e3599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Load dataset\n",
        "import pandas\n",
        "\n",
        "pwd = '/content/gdrive'\n",
        "# from google.colab import drive\n",
        "# drive.mount(pwd)\n",
        "\n",
        "df= pandas.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/Tweets.csv\", index_col=0, usecols=['tweet_id','airline_sentiment', 'text'])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         airline_sentiment                                               text\n",
              "tweet_id                                                                     \n",
              "0                  neutral                @VirginAmerica What @dhepburn said.\n",
              "1                 positive  @VirginAmerica plus you've added commercials t...\n",
              "2                  neutral  @VirginAmerica I didn't today... Must mean I n...\n",
              "3                 negative  @VirginAmerica it's really aggressive to blast...\n",
              "4                 negative  @VirginAmerica and it's a really big bad thing..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0V87lja7wQi",
        "colab_type": "text"
      },
      "source": [
        "## **General text preprocessing**\n",
        "\n",
        "This process references the following repo: https://github.com/sebsk/CS224N-Project/blob/df0050357d40e7f46b9c421ade52cdb9358c831c/Text_preprocessing.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fem6Y7fp45I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove URL, RT, mention(@)\n",
        "\n",
        "df.text = df.text.str.replace(r'http(\\S)+', r'')\n",
        "df.text = df.text.str.replace(r'http ...', r'')\n",
        "df.text = df.text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
        "df.text = df.text.str.replace(r'@[\\S]+',r'')\n",
        "\n",
        "# Remove non-ascii words or characters\n",
        "df.text = [''.join([i if ord(i) < 128 else '' for i in text]) for text in df.text]\n",
        "df.text = df.text.str.replace(r'_[\\S]?',r'')\n",
        "\n",
        "# Remove extra space\n",
        "df.text = df.text.str.replace(r'[ ]{2, }',r' ')\n",
        "\n",
        "# Remove &, < and >\n",
        "df.text = df.text.str.replace(r'&amp;?',r'and')\n",
        "df.text = df.text.str.replace(r'&lt;',r'<')\n",
        "df.text = df.text.str.replace(r'&gt;',r'>')\n",
        "\n",
        "# Insert space between words and punctuation marks\n",
        "df.text = df.text.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
        "df.text = df.text.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
        "\n",
        "# Lowercased and strip\n",
        "df.text = df.text.str.lower()\n",
        "df.text = df.text.str.strip()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijLAhhi7-eot",
        "colab_type": "code",
        "outputId": "832eb1a4-14da-48dd-bf97-b818cb2a55b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df['text_length'] = [len(text.split(' ')) for text in df.text]\n",
        "print(df.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14640, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FD6eleJ_fs_",
        "colab_type": "code",
        "outputId": "3999588e-dce5-4c4c-90ed-c5898546a455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Drop texts with length <=3 and drop duplicates\n",
        "df = df[df['text_length']>3]\n",
        "df = df.drop_duplicates(subset=['text'])\n",
        "\n",
        "print(df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13977, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9wWD8K18vFq",
        "colab_type": "code",
        "outputId": "0da8daa4-b2f8-4573-eec7-f1e27f5045b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Summary of sample size and labels\n",
        "df.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn87hLcnAmBC",
        "colab_type": "code",
        "outputId": "668091d6-228b-4323-9fa7-eb99f1828a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df.airline_sentiment.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    8998\n",
              "neutral     2834\n",
              "positive    2145\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueTitcLuAwJt",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocess text into the BERT format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2tkgNNBCy_y",
        "colab_type": "code",
        "outputId": "da34c3db-3e26-433d-da69-6fc2112babd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df['BERT_processed_text'] = '[CLS] '+df.text\n",
        "df.BERT_processed_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id\n",
              "0                                       [CLS] what  said .\n",
              "1        [CLS] plus you ' ve added commercials to the e...\n",
              "2        [CLS] i didn ' t today ... must mean i need to...\n",
              "3        [CLS] it ' s really aggressive to blast obnoxi...\n",
              "4         [CLS] and it ' s a really big bad thing about it\n",
              "                               ...                        \n",
              "14635    [CLS] thank you we got on a different flight t...\n",
              "14636    [CLS] leaving over 20 minutes late flight . no...\n",
              "14637    [CLS] please bring american airlines to # blac...\n",
              "14638    [CLS] you have my money , you change my flight...\n",
              "14639    [CLS] we have 8 ppl so we need 2 know how many...\n",
              "Name: BERT_processed_text, Length: 13977, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuJqt4MpBCEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "df['BERT_processed_text_length'] = [len(tokenizer.tokenize(sent)) for sent in df.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPVCqB0tBt4X",
        "colab_type": "code",
        "outputId": "f34d48ab-f66b-4b65-daa2-d1f3bd9bbff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df.BERT_processed_text_length"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id\n",
              "0         3\n",
              "1        15\n",
              "2        17\n",
              "3        25\n",
              "4        11\n",
              "         ..\n",
              "14635    11\n",
              "14636    27\n",
              "14637     8\n",
              "14638    29\n",
              "14639    34\n",
              "Name: BERT_processed_text_length, Length: 13977, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "322XPAGECFm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = dict()\n",
        "for i, l in enumerate(list(df.airline_sentiment.value_counts().keys())):\n",
        "    label_dict.update({l: i})\n",
        "\n",
        "df['airline_sentiment_label'] = [label_dict[label] for label in df.airline_sentiment]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No1XpzffCN0p",
        "colab_type": "code",
        "outputId": "a669b329-7e17-4049-9adc-18ba676d8724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df.airline_sentiment_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id\n",
              "0        1\n",
              "1        2\n",
              "2        1\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "14635    2\n",
              "14636    0\n",
              "14637    1\n",
              "14638    0\n",
              "14639    1\n",
              "Name: airline_sentiment_label, Length: 13977, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qoRtDtCmL5",
        "colab_type": "text"
      },
      "source": [
        "## **Save data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri0wosWqCZox",
        "colab_type": "code",
        "outputId": "69c7296c-f246-4d65-ee33-3cf0e9a38f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# !touch /content/gdrive/My\\ Drive/Colab\\ Notebooks/bert_processed_twitter_airline_sentiment.csv\n",
        "!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks\n",
        "df.to_csv(pwd + '/My Drive/Colab Notebooks/bert_processed_twitter_airline_sentiment.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " bert_processed_twitter_airline_sentiment.csv\n",
            " BERT-sentiment-analysis.ipynb\n",
            "'Copy of project2_github.ipynb'\n",
            "'Copy of project2_world_bank.ipynb'\n",
            "'Copy of VideoColorizerColab.ipynb'\n",
            " DeOldify_colab.ipynb\n",
            " Tweets.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vms5tcyhBAhw",
        "colab_type": "text"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM5juB_dAuSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEa5QgoZA7iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training params\n",
        "label_names = ['positive', 'negative', 'neutral']\n",
        "model_name = 'st-sentiment'\n",
        "device = torch.device(\"cuda:0\")\n",
        "bert_size = 'bert-base-uncased'\n",
        "\n",
        "train_batch_size = 32 # batch size\n",
        "clip_grad = 1.0 # gradient clipping\n",
        "log_every = 10 # number of mini-batches before logging\n",
        "max_epoch = 100 # max number of epochs\n",
        "max_patience = 3 # number of iterations to wait before decaying learning rate\n",
        "max_num_trial = 3 # number of trials before terminating training\n",
        "lr_decay = 0.5 # learning rate decay\n",
        "lr_bert = 0.00002 # BERT learning rate\n",
        "lr = 0.001 # learning rate\n",
        "valid_niter = 500 # perform validation after n iterations\n",
        "dropout = 0.3 # dropout rate\n",
        "verbose = True\n",
        "\n",
        "prefix = model_name + '_' + bert_size\n",
        "model_save_path = pwd + '/My Drive/Colab Notebooks/' + prefix+'_model.bin'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw9XV4I7y70J",
        "colab_type": "code",
        "outputId": "79deddad-9bb8-40c5-cf42-d3f2f71922f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Split up data into train and validation, where validation is 20% of the dataset\n",
        "training_data,validation_data = train_test_split(df,test_size=0.2,random_state=42)\n",
        "print(len(df), len(training_data), len(validation_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13977 11181 2796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gq8RtDNzoB2",
        "colab_type": "code",
        "outputId": "11be745a-5082-4ca4-fefd-3e00972bdd49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(training_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         airline_sentiment  ... airline_sentiment_label\n",
            "tweet_id                    ...                        \n",
            "14597             negative  ...                       0\n",
            "3167              positive  ...                       2\n",
            "13635             negative  ...                       0\n",
            "14048             negative  ...                       0\n",
            "8073              negative  ...                       0\n",
            "...                    ...  ...                     ...\n",
            "5371              negative  ...                       0\n",
            "14068             negative  ...                       0\n",
            "5575               neutral  ...                       1\n",
            "897                neutral  ...                       1\n",
            "7563              positive  ...                       2\n",
            "\n",
            "[11181 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R244ZOTfzpjs",
        "colab_type": "code",
        "outputId": "67dc1d2a-f12c-4c6a-de5f-9c23c54b65a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "train_label = dict(training_data.airline_sentiment_label.value_counts())\n",
        "label_max = float(max(train_label.values()))\n",
        "train_label_weight = torch.tensor([label_max/train_label[i] for i in range(len(train_label))], device=device)\n",
        "\n",
        "pp.pprint(train_label_weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0000, 3.2735, 4.2780], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXzs7hoFzq0L",
        "colab_type": "code",
        "outputId": "bccd80ef-ec1d-4d2a-c048-dc9b7f70ca41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Set up model and optimizer\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "model = SentimentClassifierModel(bert_size, device, len(label_names))\n",
        "optimizer = AdamW([\n",
        "        {'params': model.bert.bert.parameters()},\n",
        "        {'params': model.bert.classifier.parameters(), 'lr': float(lr)}\n",
        "    ], lr=float(lr_bert))\n",
        "\n",
        "model = model.to(device)\n",
        "print('Use device: %s' % device, file=sys.stderr)\n",
        "print('Done! time elapsed %.2f sec' % (time.time() - start_time), file=sys.stderr)\n",
        "print('-' * 80, file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cuda:0\n",
            "Done! time elapsed 17.86 sec\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz4gmK9Dz9Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Util functions for training\n",
        "import math\n",
        "import logging\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import sys\n",
        "from docopt import docopt\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, \\\n",
        "    f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def batch_iter(data, batch_size, shuffle=False, bert=None):\n",
        "    \"\"\" Yield batches of sentences and labels reverse sorted by length (largest to smallest).\n",
        "    @param data (dataframe): dataframe with ProcessedText (str) and label (int) columns\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    @param bert (str): whether for BERT training. Values: \"large\", \"base\", None\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(data.shape[0] / batch_size)\n",
        "    index_array = list(range(data.shape[0]))\n",
        "\n",
        "    if shuffle:\n",
        "        data = data.sample(frac=1)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = data.iloc[indices].sort_values(by='BERT_processed_text_length', ascending=False)\n",
        "        sents = list(examples.BERT_processed_text)\n",
        "\n",
        "        targets = list(examples.airline_sentiment_label.values)\n",
        "        yield sents, targets  # list[list[str]] if not bert else list[str], list[int]\n",
        "        \n",
        "def validation(model, df_val, bert_size, loss_func, device):\n",
        "    \"\"\" validation of model during training.\n",
        "    @param model (nn.Module): the model being trained\n",
        "    @param df_val (dataframe): validation dataset\n",
        "    @param bert_size (str): large or base\n",
        "    @param loss_func(nn.Module): loss function\n",
        "    @param device (torch.device)\n",
        "    @return avg loss value across validation dataset\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    df_val = df_val.sort_values(by='BERT_processed_text_length', ascending=False)\n",
        "\n",
        "    ProcessedText_BERT = list(df_val.BERT_processed_text)\n",
        "    InformationType_label = list(df_val.airline_sentiment_label)\n",
        "\n",
        "    val_batch_size = 32\n",
        "\n",
        "    n_batch = int(np.ceil(df_val.shape[0]/val_batch_size))\n",
        "\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batch):\n",
        "            sents = ProcessedText_BERT[i*val_batch_size: (i+1)*val_batch_size]\n",
        "            targets = torch.tensor(InformationType_label[i*val_batch_size: (i+1)*val_batch_size],\n",
        "                                   dtype=torch.long, device=device)\n",
        "            batch_size = len(sents)\n",
        "            pre_softmax = model(sents)[0]\n",
        "            batch_loss = loss_func(pre_softmax, targets)\n",
        "            total_loss += batch_loss.item()*batch_size\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return total_loss/df_val.shape[0]\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, path='cm', cmap=plt.cm.Reds):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    pickle.dump(cm, open(path, 'wb'))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_rgVWG0atw",
        "colab_type": "code",
        "outputId": "cec13af9-2f70-4b9f-c726-37c95cc2f170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Train\n",
        "\n",
        "model.train()\n",
        "cn_loss = torch.nn.CrossEntropyLoss(weight=train_label_weight, reduction='mean')\n",
        "torch.save(cn_loss, 'loss_func')  # for later testing\n",
        "\n",
        "# Initialize training variables\n",
        "num_trial = 0\n",
        "train_iter = 0\n",
        "patience = 0\n",
        "cum_loss = 0\n",
        "report_loss = 0\n",
        "cum_examples = report_examples = epoch = 0\n",
        "hist_valid_scores = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ032hg6b3ZL",
        "colab_type": "code",
        "outputId": "7d4e03e4-3331-4dae-e59f-a94bf3ccee1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tloss_func  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj0DuEms0dJ4",
        "colab_type": "code",
        "outputId": "7efba2b6-ca9a-495e-f456-49383c82bbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "\n",
        "train_time = begin_time = time.time()\n",
        "print('Begin Maximum Likelihood training...')\n",
        "\n",
        "# Training loop\n",
        "while True:\n",
        "    epoch += 1\n",
        "    for sents, targets in batch_iter(training_data, batch_size=train_batch_size, shuffle=True, bert='base'):  # for each epoch\n",
        "        train_iter += 1\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = len(sents)\n",
        "        pre_softmax = model(sents)[0]\n",
        "\n",
        "        # Calculate loss and gradient function\n",
        "        loss = cn_loss(pre_softmax, torch.tensor(targets, dtype=torch.long, device=device))\n",
        "        loss.backward()\n",
        "\n",
        "        # Next step\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses_val = loss.item() * batch_size\n",
        "        report_loss += batch_losses_val\n",
        "        cum_loss += batch_losses_val\n",
        "\n",
        "        report_examples += batch_size\n",
        "        cum_examples += batch_size\n",
        "\n",
        "        if train_iter % log_every == 0:\n",
        "            print('epoch %d, iter %d, avg. loss %.2f, '\n",
        "                  'cum. examples %d, speed %.2f examples/sec, '\n",
        "                  'time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                     report_loss / report_examples,\n",
        "                     cum_examples,\n",
        "                     report_examples / (time.time() - train_time),\n",
        "                     time.time() - begin_time), file=sys.stderr)\n",
        "\n",
        "            train_time = time.time()\n",
        "            report_loss = report_examples = 0.\n",
        "\n",
        "        # perform validation\n",
        "        if train_iter % valid_niter == 0:\n",
        "            print('epoch %d, iter %d, cum. loss %.2f, cum. examples %d' % (epoch, train_iter,\n",
        "                 cum_loss / cum_examples,\n",
        "                 cum_examples), file=sys.stderr)\n",
        "\n",
        "            cum_loss = cum_examples = 0.\n",
        "\n",
        "            print('begin validation ...', file=sys.stderr)\n",
        "\n",
        "            validation_loss = validation(model, validation_data, bert_size, cn_loss, device)   # dev batch size can be a bit larger\n",
        "\n",
        "            print('validation: iter %d, loss %f' % (train_iter, validation_loss), file=sys.stderr)\n",
        "\n",
        "            is_better = len(hist_valid_scores) == 0 or validation_loss < min(hist_valid_scores)\n",
        "            hist_valid_scores.append(validation_loss)\n",
        "\n",
        "            if is_better:\n",
        "                patience = 0\n",
        "                print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
        "\n",
        "                model.save(model_save_path)\n",
        "\n",
        "                # also save the optimizers' state\n",
        "                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "            elif patience < int(max_patience):\n",
        "                patience += 1\n",
        "                print('hit patience %d' % patience, file=sys.stderr)\n",
        "\n",
        "                if patience == int(max_patience):\n",
        "                    num_trial += 1\n",
        "                    print('hit #%d trial' % num_trial, file=sys.stderr)\n",
        "                    if num_trial == max_num_trial:\n",
        "                        print('early stop!', file=sys.stderr)\n",
        "                        exit(0)\n",
        "\n",
        "                    # decay lr, and restore from previously best checkpoint\n",
        "                    print('load previously best model and decay learning rate to %f%%' %\n",
        "                          (float(lr_decay)*100), file=sys.stderr)\n",
        "\n",
        "                    # load model\n",
        "                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "                    model.load_state_dict(params['state_dict'])\n",
        "                    model = model.to(device)\n",
        "\n",
        "                    print('restore parameters of the optimizers', file=sys.stderr)\n",
        "                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n",
        "                    # set new lr\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] *= float(lr_decay)\n",
        "\n",
        "                    # reset patience\n",
        "                    patience = 0\n",
        "\n",
        "            if epoch == int(max_epoch):\n",
        "                print('reached maximum number of epochs!', file=sys.stderr)\n",
        "                exit(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Maximum Likelihood training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, iter 10, avg. loss 1.11, cum. examples 320, speed 171.93 examples/sec, time elapsed 1.86 sec\n",
            "epoch 1, iter 20, avg. loss 1.07, cum. examples 640, speed 182.67 examples/sec, time elapsed 3.61 sec\n",
            "epoch 1, iter 30, avg. loss 1.00, cum. examples 960, speed 175.84 examples/sec, time elapsed 5.43 sec\n",
            "epoch 1, iter 40, avg. loss 0.83, cum. examples 1280, speed 180.03 examples/sec, time elapsed 7.21 sec\n",
            "epoch 1, iter 50, avg. loss 0.80, cum. examples 1600, speed 175.23 examples/sec, time elapsed 9.04 sec\n",
            "epoch 1, iter 60, avg. loss 0.65, cum. examples 1920, speed 176.00 examples/sec, time elapsed 10.86 sec\n",
            "epoch 1, iter 70, avg. loss 0.86, cum. examples 2240, speed 172.25 examples/sec, time elapsed 12.71 sec\n",
            "epoch 1, iter 80, avg. loss 0.74, cum. examples 2560, speed 181.19 examples/sec, time elapsed 14.48 sec\n",
            "epoch 1, iter 90, avg. loss 0.74, cum. examples 2880, speed 178.32 examples/sec, time elapsed 16.28 sec\n",
            "epoch 1, iter 100, avg. loss 0.63, cum. examples 3200, speed 177.10 examples/sec, time elapsed 18.08 sec\n",
            "epoch 1, iter 110, avg. loss 0.64, cum. examples 3520, speed 178.59 examples/sec, time elapsed 19.87 sec\n",
            "epoch 1, iter 120, avg. loss 0.77, cum. examples 3840, speed 175.98 examples/sec, time elapsed 21.69 sec\n",
            "epoch 1, iter 130, avg. loss 0.67, cum. examples 4160, speed 179.92 examples/sec, time elapsed 23.47 sec\n",
            "epoch 1, iter 140, avg. loss 0.59, cum. examples 4480, speed 183.51 examples/sec, time elapsed 25.22 sec\n",
            "epoch 1, iter 150, avg. loss 0.63, cum. examples 4800, speed 180.24 examples/sec, time elapsed 26.99 sec\n",
            "epoch 1, iter 160, avg. loss 0.57, cum. examples 5120, speed 176.91 examples/sec, time elapsed 28.80 sec\n",
            "epoch 1, iter 170, avg. loss 0.62, cum. examples 5440, speed 184.01 examples/sec, time elapsed 30.54 sec\n",
            "epoch 1, iter 180, avg. loss 0.57, cum. examples 5760, speed 179.47 examples/sec, time elapsed 32.32 sec\n",
            "epoch 1, iter 190, avg. loss 0.51, cum. examples 6080, speed 182.79 examples/sec, time elapsed 34.07 sec\n",
            "epoch 1, iter 200, avg. loss 0.58, cum. examples 6400, speed 183.22 examples/sec, time elapsed 35.82 sec\n",
            "epoch 1, iter 210, avg. loss 0.57, cum. examples 6720, speed 172.70 examples/sec, time elapsed 37.67 sec\n",
            "epoch 1, iter 220, avg. loss 0.61, cum. examples 7040, speed 177.20 examples/sec, time elapsed 39.48 sec\n",
            "epoch 1, iter 230, avg. loss 0.62, cum. examples 7360, speed 177.33 examples/sec, time elapsed 41.29 sec\n",
            "epoch 1, iter 240, avg. loss 0.64, cum. examples 7680, speed 181.95 examples/sec, time elapsed 43.04 sec\n",
            "epoch 1, iter 250, avg. loss 0.61, cum. examples 8000, speed 177.08 examples/sec, time elapsed 44.85 sec\n",
            "epoch 1, iter 260, avg. loss 0.57, cum. examples 8320, speed 173.91 examples/sec, time elapsed 46.69 sec\n",
            "epoch 1, iter 270, avg. loss 0.61, cum. examples 8640, speed 178.61 examples/sec, time elapsed 48.48 sec\n",
            "epoch 1, iter 280, avg. loss 0.50, cum. examples 8960, speed 183.40 examples/sec, time elapsed 50.23 sec\n",
            "epoch 1, iter 290, avg. loss 0.51, cum. examples 9280, speed 175.55 examples/sec, time elapsed 52.05 sec\n",
            "epoch 1, iter 300, avg. loss 0.51, cum. examples 9600, speed 172.24 examples/sec, time elapsed 53.91 sec\n",
            "epoch 1, iter 310, avg. loss 0.49, cum. examples 9920, speed 176.53 examples/sec, time elapsed 55.72 sec\n",
            "epoch 1, iter 320, avg. loss 0.48, cum. examples 10240, speed 178.78 examples/sec, time elapsed 57.51 sec\n",
            "epoch 1, iter 330, avg. loss 0.68, cum. examples 10560, speed 178.07 examples/sec, time elapsed 59.31 sec\n",
            "epoch 1, iter 340, avg. loss 0.66, cum. examples 10880, speed 179.30 examples/sec, time elapsed 61.09 sec\n",
            "epoch 1, iter 350, avg. loss 0.51, cum. examples 11181, speed 178.34 examples/sec, time elapsed 62.78 sec\n",
            "epoch 2, iter 360, avg. loss 0.40, cum. examples 11501, speed 171.94 examples/sec, time elapsed 64.64 sec\n",
            "epoch 2, iter 370, avg. loss 0.51, cum. examples 11821, speed 181.04 examples/sec, time elapsed 66.41 sec\n",
            "epoch 2, iter 380, avg. loss 0.54, cum. examples 12141, speed 176.70 examples/sec, time elapsed 68.22 sec\n",
            "epoch 2, iter 390, avg. loss 0.49, cum. examples 12461, speed 177.75 examples/sec, time elapsed 70.02 sec\n",
            "epoch 2, iter 400, avg. loss 0.39, cum. examples 12781, speed 177.05 examples/sec, time elapsed 71.83 sec\n",
            "epoch 2, iter 410, avg. loss 0.39, cum. examples 13101, speed 179.47 examples/sec, time elapsed 73.61 sec\n",
            "epoch 2, iter 420, avg. loss 0.37, cum. examples 13421, speed 181.18 examples/sec, time elapsed 75.38 sec\n",
            "epoch 2, iter 430, avg. loss 0.32, cum. examples 13741, speed 175.94 examples/sec, time elapsed 77.20 sec\n",
            "epoch 2, iter 440, avg. loss 0.42, cum. examples 14061, speed 172.94 examples/sec, time elapsed 79.05 sec\n",
            "epoch 2, iter 450, avg. loss 0.39, cum. examples 14381, speed 186.32 examples/sec, time elapsed 80.77 sec\n",
            "epoch 2, iter 460, avg. loss 0.38, cum. examples 14701, speed 179.89 examples/sec, time elapsed 82.55 sec\n",
            "epoch 2, iter 470, avg. loss 0.49, cum. examples 15021, speed 171.68 examples/sec, time elapsed 84.41 sec\n",
            "epoch 2, iter 480, avg. loss 0.40, cum. examples 15341, speed 179.98 examples/sec, time elapsed 86.19 sec\n",
            "epoch 2, iter 490, avg. loss 0.43, cum. examples 15661, speed 181.68 examples/sec, time elapsed 87.95 sec\n",
            "epoch 2, iter 500, avg. loss 0.37, cum. examples 15981, speed 179.72 examples/sec, time elapsed 89.73 sec\n",
            "epoch 2, iter 500, cum. loss 0.59, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 500, loss 0.548059\n",
            "save currently the best model to [/content/gdrive/My Drive/Colab Notebooks/st-sentiment_bert-base-uncased_model.bin]\n",
            "save model parameters to [/content/gdrive/My Drive/Colab Notebooks/st-sentiment_bert-base-uncased_model.bin]\n",
            "epoch 2, iter 510, avg. loss 0.39, cum. examples 320, speed 32.16 examples/sec, time elapsed 99.68 sec\n",
            "epoch 2, iter 520, avg. loss 0.53, cum. examples 640, speed 183.17 examples/sec, time elapsed 101.43 sec\n",
            "epoch 2, iter 530, avg. loss 0.47, cum. examples 960, speed 174.34 examples/sec, time elapsed 103.26 sec\n",
            "epoch 2, iter 540, avg. loss 0.45, cum. examples 1280, speed 171.53 examples/sec, time elapsed 105.13 sec\n",
            "epoch 2, iter 550, avg. loss 0.43, cum. examples 1600, speed 170.69 examples/sec, time elapsed 107.00 sec\n",
            "epoch 2, iter 560, avg. loss 0.32, cum. examples 1920, speed 174.45 examples/sec, time elapsed 108.84 sec\n",
            "epoch 2, iter 570, avg. loss 0.42, cum. examples 2240, speed 171.18 examples/sec, time elapsed 110.71 sec\n",
            "epoch 2, iter 580, avg. loss 0.45, cum. examples 2560, speed 176.93 examples/sec, time elapsed 112.52 sec\n",
            "epoch 2, iter 590, avg. loss 0.45, cum. examples 2880, speed 175.01 examples/sec, time elapsed 114.35 sec\n",
            "epoch 2, iter 600, avg. loss 0.55, cum. examples 3200, speed 181.29 examples/sec, time elapsed 116.11 sec\n",
            "epoch 2, iter 610, avg. loss 0.49, cum. examples 3520, speed 177.78 examples/sec, time elapsed 117.91 sec\n",
            "epoch 2, iter 620, avg. loss 0.52, cum. examples 3840, speed 178.40 examples/sec, time elapsed 119.71 sec\n",
            "epoch 2, iter 630, avg. loss 0.40, cum. examples 4160, speed 181.11 examples/sec, time elapsed 121.47 sec\n",
            "epoch 2, iter 640, avg. loss 0.43, cum. examples 4480, speed 175.84 examples/sec, time elapsed 123.29 sec\n",
            "epoch 2, iter 650, avg. loss 0.40, cum. examples 4800, speed 187.53 examples/sec, time elapsed 125.00 sec\n",
            "epoch 2, iter 660, avg. loss 0.39, cum. examples 5120, speed 182.94 examples/sec, time elapsed 126.75 sec\n",
            "epoch 2, iter 670, avg. loss 0.44, cum. examples 5440, speed 176.11 examples/sec, time elapsed 128.57 sec\n",
            "epoch 2, iter 680, avg. loss 0.45, cum. examples 5760, speed 179.32 examples/sec, time elapsed 130.35 sec\n",
            "epoch 2, iter 690, avg. loss 0.40, cum. examples 6080, speed 176.70 examples/sec, time elapsed 132.16 sec\n",
            "epoch 2, iter 700, avg. loss 0.47, cum. examples 6381, speed 179.49 examples/sec, time elapsed 133.84 sec\n",
            "epoch 3, iter 710, avg. loss 0.28, cum. examples 6701, speed 178.47 examples/sec, time elapsed 135.63 sec\n",
            "epoch 3, iter 720, avg. loss 0.26, cum. examples 7021, speed 181.15 examples/sec, time elapsed 137.40 sec\n",
            "epoch 3, iter 730, avg. loss 0.32, cum. examples 7341, speed 174.30 examples/sec, time elapsed 139.24 sec\n",
            "epoch 3, iter 740, avg. loss 0.24, cum. examples 7661, speed 182.88 examples/sec, time elapsed 140.99 sec\n",
            "epoch 3, iter 750, avg. loss 0.27, cum. examples 7981, speed 183.15 examples/sec, time elapsed 142.73 sec\n",
            "epoch 3, iter 760, avg. loss 0.20, cum. examples 8301, speed 177.31 examples/sec, time elapsed 144.54 sec\n",
            "epoch 3, iter 770, avg. loss 0.34, cum. examples 8621, speed 187.87 examples/sec, time elapsed 146.24 sec\n",
            "epoch 3, iter 780, avg. loss 0.31, cum. examples 8941, speed 177.35 examples/sec, time elapsed 148.05 sec\n",
            "epoch 3, iter 790, avg. loss 0.23, cum. examples 9261, speed 173.74 examples/sec, time elapsed 149.89 sec\n",
            "epoch 3, iter 800, avg. loss 0.39, cum. examples 9581, speed 175.18 examples/sec, time elapsed 151.72 sec\n",
            "epoch 3, iter 810, avg. loss 0.30, cum. examples 9901, speed 182.23 examples/sec, time elapsed 153.47 sec\n",
            "epoch 3, iter 820, avg. loss 0.37, cum. examples 10221, speed 182.79 examples/sec, time elapsed 155.22 sec\n",
            "epoch 3, iter 830, avg. loss 0.30, cum. examples 10541, speed 177.90 examples/sec, time elapsed 157.02 sec\n",
            "epoch 3, iter 840, avg. loss 0.27, cum. examples 10861, speed 179.52 examples/sec, time elapsed 158.81 sec\n",
            "epoch 3, iter 850, avg. loss 0.29, cum. examples 11181, speed 176.86 examples/sec, time elapsed 160.61 sec\n",
            "epoch 3, iter 860, avg. loss 0.22, cum. examples 11501, speed 177.15 examples/sec, time elapsed 162.42 sec\n",
            "epoch 3, iter 870, avg. loss 0.29, cum. examples 11821, speed 174.16 examples/sec, time elapsed 164.26 sec\n",
            "epoch 3, iter 880, avg. loss 0.38, cum. examples 12141, speed 179.90 examples/sec, time elapsed 166.04 sec\n",
            "epoch 3, iter 890, avg. loss 0.35, cum. examples 12461, speed 168.80 examples/sec, time elapsed 167.93 sec\n",
            "epoch 3, iter 900, avg. loss 0.29, cum. examples 12781, speed 171.86 examples/sec, time elapsed 169.80 sec\n",
            "epoch 3, iter 910, avg. loss 0.36, cum. examples 13101, speed 178.91 examples/sec, time elapsed 171.59 sec\n",
            "epoch 3, iter 920, avg. loss 0.41, cum. examples 13421, speed 177.06 examples/sec, time elapsed 173.39 sec\n",
            "epoch 3, iter 930, avg. loss 0.41, cum. examples 13741, speed 177.27 examples/sec, time elapsed 175.20 sec\n",
            "epoch 3, iter 940, avg. loss 0.35, cum. examples 14061, speed 182.24 examples/sec, time elapsed 176.96 sec\n",
            "epoch 3, iter 950, avg. loss 0.35, cum. examples 14381, speed 176.66 examples/sec, time elapsed 178.77 sec\n",
            "epoch 3, iter 960, avg. loss 0.29, cum. examples 14701, speed 176.41 examples/sec, time elapsed 180.58 sec\n",
            "epoch 3, iter 970, avg. loss 0.37, cum. examples 15021, speed 179.91 examples/sec, time elapsed 182.36 sec\n",
            "epoch 3, iter 980, avg. loss 0.30, cum. examples 15341, speed 173.31 examples/sec, time elapsed 184.21 sec\n",
            "epoch 3, iter 990, avg. loss 0.27, cum. examples 15661, speed 180.22 examples/sec, time elapsed 185.98 sec\n",
            "epoch 3, iter 1000, avg. loss 0.30, cum. examples 15981, speed 184.77 examples/sec, time elapsed 187.72 sec\n",
            "epoch 3, iter 1000, cum. loss 0.36, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 1000, loss 0.657654\n",
            "hit patience 1\n",
            "epoch 3, iter 1010, avg. loss 0.30, cum. examples 320, speed 57.80 examples/sec, time elapsed 193.25 sec\n",
            "epoch 3, iter 1020, avg. loss 0.34, cum. examples 640, speed 177.45 examples/sec, time elapsed 195.06 sec\n",
            "epoch 3, iter 1030, avg. loss 0.32, cum. examples 960, speed 179.35 examples/sec, time elapsed 196.84 sec\n",
            "epoch 3, iter 1040, avg. loss 0.29, cum. examples 1280, speed 182.49 examples/sec, time elapsed 198.59 sec\n",
            "epoch 3, iter 1050, avg. loss 0.25, cum. examples 1581, speed 180.94 examples/sec, time elapsed 200.26 sec\n",
            "epoch 4, iter 1060, avg. loss 0.19, cum. examples 1901, speed 175.68 examples/sec, time elapsed 202.08 sec\n",
            "epoch 4, iter 1070, avg. loss 0.15, cum. examples 2221, speed 175.07 examples/sec, time elapsed 203.91 sec\n",
            "epoch 4, iter 1080, avg. loss 0.28, cum. examples 2541, speed 175.67 examples/sec, time elapsed 205.73 sec\n",
            "epoch 4, iter 1090, avg. loss 0.23, cum. examples 2861, speed 178.35 examples/sec, time elapsed 207.52 sec\n",
            "epoch 4, iter 1100, avg. loss 0.20, cum. examples 3181, speed 175.42 examples/sec, time elapsed 209.35 sec\n",
            "epoch 4, iter 1110, avg. loss 0.13, cum. examples 3501, speed 176.63 examples/sec, time elapsed 211.16 sec\n",
            "epoch 4, iter 1120, avg. loss 0.18, cum. examples 3821, speed 178.18 examples/sec, time elapsed 212.96 sec\n",
            "epoch 4, iter 1130, avg. loss 0.20, cum. examples 4141, speed 178.76 examples/sec, time elapsed 214.75 sec\n",
            "epoch 4, iter 1140, avg. loss 0.26, cum. examples 4461, speed 177.82 examples/sec, time elapsed 216.55 sec\n",
            "epoch 4, iter 1150, avg. loss 0.20, cum. examples 4781, speed 179.12 examples/sec, time elapsed 218.33 sec\n",
            "epoch 4, iter 1160, avg. loss 0.16, cum. examples 5101, speed 181.96 examples/sec, time elapsed 220.09 sec\n",
            "epoch 4, iter 1170, avg. loss 0.18, cum. examples 5421, speed 173.73 examples/sec, time elapsed 221.94 sec\n",
            "epoch 4, iter 1180, avg. loss 0.25, cum. examples 5741, speed 183.97 examples/sec, time elapsed 223.68 sec\n",
            "epoch 4, iter 1190, avg. loss 0.20, cum. examples 6061, speed 176.86 examples/sec, time elapsed 225.48 sec\n",
            "epoch 4, iter 1200, avg. loss 0.14, cum. examples 6381, speed 176.16 examples/sec, time elapsed 227.30 sec\n",
            "epoch 4, iter 1210, avg. loss 0.20, cum. examples 6701, speed 174.39 examples/sec, time elapsed 229.14 sec\n",
            "epoch 4, iter 1220, avg. loss 0.18, cum. examples 7021, speed 171.64 examples/sec, time elapsed 231.00 sec\n",
            "epoch 4, iter 1230, avg. loss 0.18, cum. examples 7341, speed 185.06 examples/sec, time elapsed 232.73 sec\n",
            "epoch 4, iter 1240, avg. loss 0.24, cum. examples 7661, speed 173.05 examples/sec, time elapsed 234.58 sec\n",
            "epoch 4, iter 1250, avg. loss 0.30, cum. examples 7981, speed 177.08 examples/sec, time elapsed 236.39 sec\n",
            "epoch 4, iter 1260, avg. loss 0.23, cum. examples 8301, speed 172.32 examples/sec, time elapsed 238.25 sec\n",
            "epoch 4, iter 1270, avg. loss 0.13, cum. examples 8621, speed 178.31 examples/sec, time elapsed 240.04 sec\n",
            "epoch 4, iter 1280, avg. loss 0.16, cum. examples 8941, speed 178.91 examples/sec, time elapsed 241.83 sec\n",
            "epoch 4, iter 1290, avg. loss 0.22, cum. examples 9261, speed 177.20 examples/sec, time elapsed 243.64 sec\n",
            "epoch 4, iter 1300, avg. loss 0.17, cum. examples 9581, speed 184.95 examples/sec, time elapsed 245.37 sec\n",
            "epoch 4, iter 1310, avg. loss 0.21, cum. examples 9901, speed 177.33 examples/sec, time elapsed 247.17 sec\n",
            "epoch 4, iter 1320, avg. loss 0.23, cum. examples 10221, speed 175.14 examples/sec, time elapsed 249.00 sec\n",
            "epoch 4, iter 1330, avg. loss 0.22, cum. examples 10541, speed 176.19 examples/sec, time elapsed 250.81 sec\n",
            "epoch 4, iter 1340, avg. loss 0.23, cum. examples 10861, speed 179.44 examples/sec, time elapsed 252.60 sec\n",
            "epoch 4, iter 1350, avg. loss 0.14, cum. examples 11181, speed 174.70 examples/sec, time elapsed 254.43 sec\n",
            "epoch 4, iter 1360, avg. loss 0.22, cum. examples 11501, speed 175.95 examples/sec, time elapsed 256.25 sec\n",
            "epoch 4, iter 1370, avg. loss 0.21, cum. examples 11821, speed 184.84 examples/sec, time elapsed 257.98 sec\n",
            "epoch 4, iter 1380, avg. loss 0.18, cum. examples 12141, speed 171.69 examples/sec, time elapsed 259.84 sec\n",
            "epoch 4, iter 1390, avg. loss 0.18, cum. examples 12461, speed 182.55 examples/sec, time elapsed 261.60 sec\n",
            "epoch 4, iter 1400, avg. loss 0.23, cum. examples 12762, speed 181.55 examples/sec, time elapsed 263.26 sec\n",
            "epoch 5, iter 1410, avg. loss 0.11, cum. examples 13082, speed 181.71 examples/sec, time elapsed 265.02 sec\n",
            "epoch 5, iter 1420, avg. loss 0.12, cum. examples 13402, speed 178.53 examples/sec, time elapsed 266.81 sec\n",
            "epoch 5, iter 1430, avg. loss 0.13, cum. examples 13722, speed 181.20 examples/sec, time elapsed 268.58 sec\n",
            "epoch 5, iter 1440, avg. loss 0.15, cum. examples 14042, speed 180.46 examples/sec, time elapsed 270.35 sec\n",
            "epoch 5, iter 1450, avg. loss 0.10, cum. examples 14362, speed 178.95 examples/sec, time elapsed 272.14 sec\n",
            "epoch 5, iter 1460, avg. loss 0.11, cum. examples 14682, speed 173.89 examples/sec, time elapsed 273.98 sec\n",
            "epoch 5, iter 1470, avg. loss 0.12, cum. examples 15002, speed 182.49 examples/sec, time elapsed 275.73 sec\n",
            "epoch 5, iter 1480, avg. loss 0.12, cum. examples 15322, speed 182.10 examples/sec, time elapsed 277.49 sec\n",
            "epoch 5, iter 1490, avg. loss 0.18, cum. examples 15642, speed 174.63 examples/sec, time elapsed 279.32 sec\n",
            "epoch 5, iter 1500, avg. loss 0.17, cum. examples 15962, speed 178.80 examples/sec, time elapsed 281.11 sec\n",
            "epoch 5, iter 1500, cum. loss 0.20, cum. examples 15962\n",
            "begin validation ...\n",
            "validation: iter 1500, loss 0.911084\n",
            "hit patience 2\n",
            "epoch 5, iter 1510, avg. loss 0.11, cum. examples 320, speed 57.66 examples/sec, time elapsed 286.66 sec\n",
            "epoch 5, iter 1520, avg. loss 0.17, cum. examples 640, speed 179.32 examples/sec, time elapsed 288.45 sec\n",
            "epoch 5, iter 1530, avg. loss 0.14, cum. examples 960, speed 178.21 examples/sec, time elapsed 290.24 sec\n",
            "epoch 5, iter 1540, avg. loss 0.19, cum. examples 1280, speed 174.70 examples/sec, time elapsed 292.08 sec\n",
            "epoch 5, iter 1550, avg. loss 0.15, cum. examples 1600, speed 176.23 examples/sec, time elapsed 293.89 sec\n",
            "epoch 5, iter 1560, avg. loss 0.13, cum. examples 1920, speed 181.32 examples/sec, time elapsed 295.66 sec\n",
            "epoch 5, iter 1570, avg. loss 0.17, cum. examples 2240, speed 178.30 examples/sec, time elapsed 297.45 sec\n",
            "epoch 5, iter 1580, avg. loss 0.16, cum. examples 2560, speed 180.47 examples/sec, time elapsed 299.22 sec\n",
            "epoch 5, iter 1590, avg. loss 0.09, cum. examples 2880, speed 176.50 examples/sec, time elapsed 301.04 sec\n",
            "epoch 5, iter 1600, avg. loss 0.12, cum. examples 3200, speed 177.42 examples/sec, time elapsed 302.84 sec\n",
            "epoch 5, iter 1610, avg. loss 0.08, cum. examples 3520, speed 177.26 examples/sec, time elapsed 304.65 sec\n",
            "epoch 5, iter 1620, avg. loss 0.18, cum. examples 3840, speed 185.37 examples/sec, time elapsed 306.37 sec\n",
            "epoch 5, iter 1630, avg. loss 0.21, cum. examples 4160, speed 182.55 examples/sec, time elapsed 308.13 sec\n",
            "epoch 5, iter 1640, avg. loss 0.09, cum. examples 4480, speed 178.73 examples/sec, time elapsed 309.92 sec\n",
            "epoch 5, iter 1650, avg. loss 0.19, cum. examples 4800, speed 169.82 examples/sec, time elapsed 311.80 sec\n",
            "epoch 5, iter 1660, avg. loss 0.16, cum. examples 5120, speed 180.49 examples/sec, time elapsed 313.57 sec\n",
            "epoch 5, iter 1670, avg. loss 0.10, cum. examples 5440, speed 176.41 examples/sec, time elapsed 315.39 sec\n",
            "epoch 5, iter 1680, avg. loss 0.18, cum. examples 5760, speed 173.90 examples/sec, time elapsed 317.23 sec\n",
            "epoch 5, iter 1690, avg. loss 0.14, cum. examples 6080, speed 176.75 examples/sec, time elapsed 319.04 sec\n",
            "epoch 5, iter 1700, avg. loss 0.08, cum. examples 6400, speed 180.05 examples/sec, time elapsed 320.82 sec\n",
            "epoch 5, iter 1710, avg. loss 0.11, cum. examples 6720, speed 181.54 examples/sec, time elapsed 322.58 sec\n",
            "epoch 5, iter 1720, avg. loss 0.14, cum. examples 7040, speed 183.80 examples/sec, time elapsed 324.32 sec\n",
            "epoch 5, iter 1730, avg. loss 0.18, cum. examples 7360, speed 175.13 examples/sec, time elapsed 326.15 sec\n",
            "epoch 5, iter 1740, avg. loss 0.13, cum. examples 7680, speed 178.05 examples/sec, time elapsed 327.95 sec\n",
            "epoch 5, iter 1750, avg. loss 0.11, cum. examples 7981, speed 173.96 examples/sec, time elapsed 329.68 sec\n",
            "epoch 6, iter 1760, avg. loss 0.07, cum. examples 8301, speed 178.65 examples/sec, time elapsed 331.47 sec\n",
            "epoch 6, iter 1770, avg. loss 0.08, cum. examples 8621, speed 181.02 examples/sec, time elapsed 333.24 sec\n",
            "epoch 6, iter 1780, avg. loss 0.05, cum. examples 8941, speed 176.36 examples/sec, time elapsed 335.05 sec\n",
            "epoch 6, iter 1790, avg. loss 0.11, cum. examples 9261, speed 184.01 examples/sec, time elapsed 336.79 sec\n",
            "epoch 6, iter 1800, avg. loss 0.07, cum. examples 9581, speed 172.73 examples/sec, time elapsed 338.64 sec\n",
            "epoch 6, iter 1810, avg. loss 0.09, cum. examples 9901, speed 182.85 examples/sec, time elapsed 340.39 sec\n",
            "epoch 6, iter 1820, avg. loss 0.07, cum. examples 10221, speed 175.25 examples/sec, time elapsed 342.22 sec\n",
            "epoch 6, iter 1830, avg. loss 0.06, cum. examples 10541, speed 184.21 examples/sec, time elapsed 343.96 sec\n",
            "epoch 6, iter 1840, avg. loss 0.08, cum. examples 10861, speed 177.22 examples/sec, time elapsed 345.76 sec\n",
            "epoch 6, iter 1850, avg. loss 0.10, cum. examples 11181, speed 186.59 examples/sec, time elapsed 347.48 sec\n",
            "epoch 6, iter 1860, avg. loss 0.05, cum. examples 11501, speed 175.41 examples/sec, time elapsed 349.30 sec\n",
            "epoch 6, iter 1870, avg. loss 0.04, cum. examples 11821, speed 179.90 examples/sec, time elapsed 351.08 sec\n",
            "epoch 6, iter 1880, avg. loss 0.15, cum. examples 12141, speed 177.78 examples/sec, time elapsed 352.88 sec\n",
            "epoch 6, iter 1890, avg. loss 0.05, cum. examples 12461, speed 177.70 examples/sec, time elapsed 354.68 sec\n",
            "epoch 6, iter 1900, avg. loss 0.11, cum. examples 12781, speed 183.80 examples/sec, time elapsed 356.42 sec\n",
            "epoch 6, iter 1910, avg. loss 0.07, cum. examples 13101, speed 173.97 examples/sec, time elapsed 358.26 sec\n",
            "epoch 6, iter 1920, avg. loss 0.09, cum. examples 13421, speed 185.08 examples/sec, time elapsed 359.99 sec\n",
            "epoch 6, iter 1930, avg. loss 0.05, cum. examples 13741, speed 176.41 examples/sec, time elapsed 361.81 sec\n",
            "epoch 6, iter 1940, avg. loss 0.08, cum. examples 14061, speed 180.75 examples/sec, time elapsed 363.58 sec\n",
            "epoch 6, iter 1950, avg. loss 0.07, cum. examples 14381, speed 180.22 examples/sec, time elapsed 365.35 sec\n",
            "epoch 6, iter 1960, avg. loss 0.05, cum. examples 14701, speed 175.05 examples/sec, time elapsed 367.18 sec\n",
            "epoch 6, iter 1970, avg. loss 0.07, cum. examples 15021, speed 176.38 examples/sec, time elapsed 369.00 sec\n",
            "epoch 6, iter 1980, avg. loss 0.10, cum. examples 15341, speed 181.36 examples/sec, time elapsed 370.76 sec\n",
            "epoch 6, iter 1990, avg. loss 0.18, cum. examples 15661, speed 184.82 examples/sec, time elapsed 372.49 sec\n",
            "epoch 6, iter 2000, avg. loss 0.15, cum. examples 15981, speed 177.10 examples/sec, time elapsed 374.30 sec\n",
            "epoch 6, iter 2000, cum. loss 0.11, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 2000, loss 0.836365\n",
            "hit patience 3\n",
            "hit #1 trial\n",
            "load previously best model and decay learning rate to 50.000000%\n",
            "restore parameters of the optimizers\n",
            "epoch 6, iter 2010, avg. loss 0.27, cum. examples 320, speed 41.02 examples/sec, time elapsed 382.10 sec\n",
            "epoch 6, iter 2020, avg. loss 0.34, cum. examples 640, speed 174.87 examples/sec, time elapsed 383.93 sec\n",
            "epoch 6, iter 2030, avg. loss 0.30, cum. examples 960, speed 176.88 examples/sec, time elapsed 385.74 sec\n",
            "epoch 6, iter 2040, avg. loss 0.42, cum. examples 1280, speed 178.62 examples/sec, time elapsed 387.53 sec\n",
            "epoch 6, iter 2050, avg. loss 0.37, cum. examples 1600, speed 181.26 examples/sec, time elapsed 389.30 sec\n",
            "epoch 6, iter 2060, avg. loss 0.37, cum. examples 1920, speed 168.83 examples/sec, time elapsed 391.19 sec\n",
            "epoch 6, iter 2070, avg. loss 0.29, cum. examples 2240, speed 178.33 examples/sec, time elapsed 392.99 sec\n",
            "epoch 6, iter 2080, avg. loss 0.32, cum. examples 2560, speed 173.94 examples/sec, time elapsed 394.83 sec\n",
            "epoch 6, iter 2090, avg. loss 0.30, cum. examples 2880, speed 183.66 examples/sec, time elapsed 396.57 sec\n",
            "epoch 6, iter 2100, avg. loss 0.35, cum. examples 3181, speed 180.01 examples/sec, time elapsed 398.24 sec\n",
            "epoch 7, iter 2110, avg. loss 0.32, cum. examples 3501, speed 180.09 examples/sec, time elapsed 400.02 sec\n",
            "epoch 7, iter 2120, avg. loss 0.25, cum. examples 3821, speed 178.24 examples/sec, time elapsed 401.82 sec\n",
            "epoch 7, iter 2130, avg. loss 0.27, cum. examples 4141, speed 175.62 examples/sec, time elapsed 403.64 sec\n",
            "epoch 7, iter 2140, avg. loss 0.29, cum. examples 4461, speed 181.71 examples/sec, time elapsed 405.40 sec\n",
            "epoch 7, iter 2150, avg. loss 0.35, cum. examples 4781, speed 175.71 examples/sec, time elapsed 407.22 sec\n",
            "epoch 7, iter 2160, avg. loss 0.36, cum. examples 5101, speed 178.43 examples/sec, time elapsed 409.02 sec\n",
            "epoch 7, iter 2170, avg. loss 0.27, cum. examples 5421, speed 183.41 examples/sec, time elapsed 410.76 sec\n",
            "epoch 7, iter 2180, avg. loss 0.48, cum. examples 5741, speed 184.19 examples/sec, time elapsed 412.50 sec\n",
            "epoch 7, iter 2190, avg. loss 0.34, cum. examples 6061, speed 181.59 examples/sec, time elapsed 414.26 sec\n",
            "epoch 7, iter 2200, avg. loss 0.32, cum. examples 6381, speed 176.08 examples/sec, time elapsed 416.08 sec\n",
            "epoch 7, iter 2210, avg. loss 0.40, cum. examples 6701, speed 184.13 examples/sec, time elapsed 417.82 sec\n",
            "epoch 7, iter 2220, avg. loss 0.35, cum. examples 7021, speed 167.38 examples/sec, time elapsed 419.73 sec\n",
            "epoch 7, iter 2230, avg. loss 0.33, cum. examples 7341, speed 169.82 examples/sec, time elapsed 421.62 sec\n",
            "epoch 7, iter 2240, avg. loss 0.39, cum. examples 7661, speed 181.73 examples/sec, time elapsed 423.38 sec\n",
            "epoch 7, iter 2250, avg. loss 0.37, cum. examples 7981, speed 175.93 examples/sec, time elapsed 425.20 sec\n",
            "epoch 7, iter 2260, avg. loss 0.31, cum. examples 8301, speed 181.24 examples/sec, time elapsed 426.96 sec\n",
            "epoch 7, iter 2270, avg. loss 0.30, cum. examples 8621, speed 184.81 examples/sec, time elapsed 428.69 sec\n",
            "epoch 7, iter 2280, avg. loss 0.32, cum. examples 8941, speed 180.90 examples/sec, time elapsed 430.46 sec\n",
            "epoch 7, iter 2290, avg. loss 0.33, cum. examples 9261, speed 179.30 examples/sec, time elapsed 432.25 sec\n",
            "epoch 7, iter 2300, avg. loss 0.36, cum. examples 9581, speed 175.96 examples/sec, time elapsed 434.07 sec\n",
            "epoch 7, iter 2310, avg. loss 0.29, cum. examples 9901, speed 177.34 examples/sec, time elapsed 435.87 sec\n",
            "epoch 7, iter 2320, avg. loss 0.32, cum. examples 10221, speed 172.06 examples/sec, time elapsed 437.73 sec\n",
            "epoch 7, iter 2330, avg. loss 0.28, cum. examples 10541, speed 183.69 examples/sec, time elapsed 439.47 sec\n",
            "epoch 7, iter 2340, avg. loss 0.29, cum. examples 10861, speed 173.22 examples/sec, time elapsed 441.32 sec\n",
            "epoch 7, iter 2350, avg. loss 0.35, cum. examples 11181, speed 176.87 examples/sec, time elapsed 443.13 sec\n",
            "epoch 7, iter 2360, avg. loss 0.31, cum. examples 11501, speed 177.98 examples/sec, time elapsed 444.93 sec\n",
            "epoch 7, iter 2370, avg. loss 0.33, cum. examples 11821, speed 176.02 examples/sec, time elapsed 446.75 sec\n",
            "epoch 7, iter 2380, avg. loss 0.27, cum. examples 12141, speed 175.90 examples/sec, time elapsed 448.57 sec\n",
            "epoch 7, iter 2390, avg. loss 0.34, cum. examples 12461, speed 175.98 examples/sec, time elapsed 450.39 sec\n",
            "epoch 7, iter 2400, avg. loss 0.27, cum. examples 12781, speed 178.56 examples/sec, time elapsed 452.18 sec\n",
            "epoch 7, iter 2410, avg. loss 0.32, cum. examples 13101, speed 179.14 examples/sec, time elapsed 453.97 sec\n",
            "epoch 7, iter 2420, avg. loss 0.28, cum. examples 13421, speed 176.47 examples/sec, time elapsed 455.78 sec\n",
            "epoch 7, iter 2430, avg. loss 0.33, cum. examples 13741, speed 183.50 examples/sec, time elapsed 457.52 sec\n",
            "epoch 7, iter 2440, avg. loss 0.31, cum. examples 14061, speed 182.12 examples/sec, time elapsed 459.28 sec\n",
            "epoch 7, iter 2450, avg. loss 0.31, cum. examples 14362, speed 177.86 examples/sec, time elapsed 460.97 sec\n",
            "epoch 8, iter 2460, avg. loss 0.25, cum. examples 14682, speed 177.16 examples/sec, time elapsed 462.78 sec\n",
            "epoch 8, iter 2470, avg. loss 0.23, cum. examples 15002, speed 180.17 examples/sec, time elapsed 464.56 sec\n",
            "epoch 8, iter 2480, avg. loss 0.22, cum. examples 15322, speed 176.70 examples/sec, time elapsed 466.37 sec\n",
            "epoch 8, iter 2490, avg. loss 0.31, cum. examples 15642, speed 179.15 examples/sec, time elapsed 468.15 sec\n",
            "epoch 8, iter 2500, avg. loss 0.24, cum. examples 15962, speed 178.91 examples/sec, time elapsed 469.94 sec\n",
            "epoch 8, iter 2500, cum. loss 0.32, cum. examples 15962\n",
            "begin validation ...\n",
            "validation: iter 2500, loss 0.563074\n",
            "hit patience 1\n",
            "epoch 8, iter 2510, avg. loss 0.27, cum. examples 320, speed 57.83 examples/sec, time elapsed 475.48 sec\n",
            "epoch 8, iter 2520, avg. loss 0.22, cum. examples 640, speed 186.31 examples/sec, time elapsed 477.19 sec\n",
            "epoch 8, iter 2530, avg. loss 0.16, cum. examples 960, speed 174.96 examples/sec, time elapsed 479.02 sec\n",
            "epoch 8, iter 2540, avg. loss 0.18, cum. examples 1280, speed 178.54 examples/sec, time elapsed 480.82 sec\n",
            "epoch 8, iter 2550, avg. loss 0.16, cum. examples 1600, speed 175.54 examples/sec, time elapsed 482.64 sec\n",
            "epoch 8, iter 2560, avg. loss 0.22, cum. examples 1920, speed 179.77 examples/sec, time elapsed 484.42 sec\n",
            "epoch 8, iter 2570, avg. loss 0.25, cum. examples 2240, speed 176.14 examples/sec, time elapsed 486.24 sec\n",
            "epoch 8, iter 2580, avg. loss 0.17, cum. examples 2560, speed 169.71 examples/sec, time elapsed 488.12 sec\n",
            "epoch 8, iter 2590, avg. loss 0.18, cum. examples 2880, speed 178.13 examples/sec, time elapsed 489.92 sec\n",
            "epoch 8, iter 2600, avg. loss 0.26, cum. examples 3200, speed 184.17 examples/sec, time elapsed 491.66 sec\n",
            "epoch 8, iter 2610, avg. loss 0.21, cum. examples 3520, speed 184.41 examples/sec, time elapsed 493.39 sec\n",
            "epoch 8, iter 2620, avg. loss 0.26, cum. examples 3840, speed 174.95 examples/sec, time elapsed 495.22 sec\n",
            "epoch 8, iter 2630, avg. loss 0.20, cum. examples 4160, speed 180.25 examples/sec, time elapsed 497.00 sec\n",
            "epoch 8, iter 2640, avg. loss 0.14, cum. examples 4480, speed 179.03 examples/sec, time elapsed 498.78 sec\n",
            "epoch 8, iter 2650, avg. loss 0.23, cum. examples 4800, speed 179.71 examples/sec, time elapsed 500.56 sec\n",
            "epoch 8, iter 2660, avg. loss 0.34, cum. examples 5120, speed 180.29 examples/sec, time elapsed 502.34 sec\n",
            "epoch 8, iter 2670, avg. loss 0.26, cum. examples 5440, speed 179.39 examples/sec, time elapsed 504.12 sec\n",
            "epoch 8, iter 2680, avg. loss 0.26, cum. examples 5760, speed 179.12 examples/sec, time elapsed 505.91 sec\n",
            "epoch 8, iter 2690, avg. loss 0.25, cum. examples 6080, speed 176.10 examples/sec, time elapsed 507.73 sec\n",
            "epoch 8, iter 2700, avg. loss 0.19, cum. examples 6400, speed 169.06 examples/sec, time elapsed 509.62 sec\n",
            "epoch 8, iter 2710, avg. loss 0.18, cum. examples 6720, speed 176.28 examples/sec, time elapsed 511.44 sec\n",
            "epoch 8, iter 2720, avg. loss 0.21, cum. examples 7040, speed 172.98 examples/sec, time elapsed 513.29 sec\n",
            "epoch 8, iter 2730, avg. loss 0.27, cum. examples 7360, speed 180.46 examples/sec, time elapsed 515.06 sec\n",
            "epoch 8, iter 2740, avg. loss 0.26, cum. examples 7680, speed 178.39 examples/sec, time elapsed 516.85 sec\n",
            "epoch 8, iter 2750, avg. loss 0.24, cum. examples 8000, speed 177.27 examples/sec, time elapsed 518.66 sec\n",
            "epoch 8, iter 2760, avg. loss 0.27, cum. examples 8320, speed 177.30 examples/sec, time elapsed 520.47 sec\n",
            "epoch 8, iter 2770, avg. loss 0.17, cum. examples 8640, speed 167.96 examples/sec, time elapsed 522.37 sec\n",
            "epoch 8, iter 2780, avg. loss 0.30, cum. examples 8960, speed 177.16 examples/sec, time elapsed 524.18 sec\n",
            "epoch 8, iter 2790, avg. loss 0.22, cum. examples 9280, speed 175.73 examples/sec, time elapsed 526.00 sec\n",
            "epoch 8, iter 2800, avg. loss 0.23, cum. examples 9581, speed 176.40 examples/sec, time elapsed 527.71 sec\n",
            "epoch 9, iter 2810, avg. loss 0.19, cum. examples 9901, speed 179.36 examples/sec, time elapsed 529.49 sec\n",
            "epoch 9, iter 2820, avg. loss 0.14, cum. examples 10221, speed 175.74 examples/sec, time elapsed 531.31 sec\n",
            "epoch 9, iter 2830, avg. loss 0.11, cum. examples 10541, speed 174.94 examples/sec, time elapsed 533.14 sec\n",
            "epoch 9, iter 2840, avg. loss 0.15, cum. examples 10861, speed 169.01 examples/sec, time elapsed 535.03 sec\n",
            "epoch 9, iter 2850, avg. loss 0.16, cum. examples 11181, speed 178.91 examples/sec, time elapsed 536.82 sec\n",
            "epoch 9, iter 2860, avg. loss 0.11, cum. examples 11501, speed 176.57 examples/sec, time elapsed 538.64 sec\n",
            "epoch 9, iter 2870, avg. loss 0.12, cum. examples 11821, speed 178.53 examples/sec, time elapsed 540.43 sec\n",
            "epoch 9, iter 2880, avg. loss 0.19, cum. examples 12141, speed 173.66 examples/sec, time elapsed 542.27 sec\n",
            "epoch 9, iter 2890, avg. loss 0.17, cum. examples 12461, speed 180.43 examples/sec, time elapsed 544.05 sec\n",
            "epoch 9, iter 2900, avg. loss 0.12, cum. examples 12781, speed 178.29 examples/sec, time elapsed 545.84 sec\n",
            "epoch 9, iter 2910, avg. loss 0.21, cum. examples 13101, speed 174.49 examples/sec, time elapsed 547.67 sec\n",
            "epoch 9, iter 2920, avg. loss 0.15, cum. examples 13421, speed 176.95 examples/sec, time elapsed 549.48 sec\n",
            "epoch 9, iter 2930, avg. loss 0.12, cum. examples 13741, speed 183.47 examples/sec, time elapsed 551.23 sec\n",
            "epoch 9, iter 2940, avg. loss 0.14, cum. examples 14061, speed 180.71 examples/sec, time elapsed 553.00 sec\n",
            "epoch 9, iter 2950, avg. loss 0.14, cum. examples 14381, speed 183.00 examples/sec, time elapsed 554.75 sec\n",
            "epoch 9, iter 2960, avg. loss 0.19, cum. examples 14701, speed 173.44 examples/sec, time elapsed 556.59 sec\n",
            "epoch 9, iter 2970, avg. loss 0.08, cum. examples 15021, speed 180.24 examples/sec, time elapsed 558.37 sec\n",
            "epoch 9, iter 2980, avg. loss 0.14, cum. examples 15341, speed 178.37 examples/sec, time elapsed 560.16 sec\n",
            "epoch 9, iter 2990, avg. loss 0.17, cum. examples 15661, speed 173.75 examples/sec, time elapsed 562.00 sec\n",
            "epoch 9, iter 3000, avg. loss 0.19, cum. examples 15981, speed 179.45 examples/sec, time elapsed 563.79 sec\n",
            "epoch 9, iter 3000, cum. loss 0.20, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 3000, loss 0.688240\n",
            "hit patience 2\n",
            "epoch 9, iter 3010, avg. loss 0.16, cum. examples 320, speed 57.65 examples/sec, time elapsed 569.34 sec\n",
            "epoch 9, iter 3020, avg. loss 0.17, cum. examples 640, speed 176.52 examples/sec, time elapsed 571.15 sec\n",
            "epoch 9, iter 3030, avg. loss 0.13, cum. examples 960, speed 180.93 examples/sec, time elapsed 572.92 sec\n",
            "epoch 9, iter 3040, avg. loss 0.15, cum. examples 1280, speed 179.65 examples/sec, time elapsed 574.70 sec\n",
            "epoch 9, iter 3050, avg. loss 0.25, cum. examples 1600, speed 185.23 examples/sec, time elapsed 576.43 sec\n",
            "epoch 9, iter 3060, avg. loss 0.18, cum. examples 1920, speed 181.16 examples/sec, time elapsed 578.20 sec\n",
            "epoch 9, iter 3070, avg. loss 0.15, cum. examples 2240, speed 177.61 examples/sec, time elapsed 580.00 sec\n",
            "epoch 9, iter 3080, avg. loss 0.23, cum. examples 2560, speed 185.00 examples/sec, time elapsed 581.73 sec\n",
            "epoch 9, iter 3090, avg. loss 0.16, cum. examples 2880, speed 169.10 examples/sec, time elapsed 583.62 sec\n",
            "epoch 9, iter 3100, avg. loss 0.18, cum. examples 3200, speed 173.15 examples/sec, time elapsed 585.47 sec\n",
            "epoch 9, iter 3110, avg. loss 0.19, cum. examples 3520, speed 182.31 examples/sec, time elapsed 587.22 sec\n",
            "epoch 9, iter 3120, avg. loss 0.16, cum. examples 3840, speed 181.47 examples/sec, time elapsed 588.99 sec\n",
            "epoch 9, iter 3130, avg. loss 0.19, cum. examples 4160, speed 182.85 examples/sec, time elapsed 590.74 sec\n",
            "epoch 9, iter 3140, avg. loss 0.16, cum. examples 4480, speed 178.49 examples/sec, time elapsed 592.53 sec\n",
            "epoch 9, iter 3150, avg. loss 0.17, cum. examples 4781, speed 168.57 examples/sec, time elapsed 594.32 sec\n",
            "epoch 10, iter 3160, avg. loss 0.09, cum. examples 5101, speed 168.92 examples/sec, time elapsed 596.21 sec\n",
            "epoch 10, iter 3170, avg. loss 0.11, cum. examples 5421, speed 177.34 examples/sec, time elapsed 598.02 sec\n",
            "epoch 10, iter 3180, avg. loss 0.09, cum. examples 5741, speed 181.36 examples/sec, time elapsed 599.78 sec\n",
            "epoch 10, iter 3190, avg. loss 0.11, cum. examples 6061, speed 175.28 examples/sec, time elapsed 601.61 sec\n",
            "epoch 10, iter 3200, avg. loss 0.05, cum. examples 6381, speed 172.05 examples/sec, time elapsed 603.47 sec\n",
            "epoch 10, iter 3210, avg. loss 0.12, cum. examples 6701, speed 180.95 examples/sec, time elapsed 605.23 sec\n",
            "epoch 10, iter 3220, avg. loss 0.12, cum. examples 7021, speed 171.91 examples/sec, time elapsed 607.10 sec\n",
            "epoch 10, iter 3230, avg. loss 0.07, cum. examples 7341, speed 177.33 examples/sec, time elapsed 608.90 sec\n",
            "epoch 10, iter 3240, avg. loss 0.11, cum. examples 7661, speed 179.48 examples/sec, time elapsed 610.68 sec\n",
            "epoch 10, iter 3250, avg. loss 0.08, cum. examples 7981, speed 181.27 examples/sec, time elapsed 612.45 sec\n",
            "epoch 10, iter 3260, avg. loss 0.08, cum. examples 8301, speed 183.08 examples/sec, time elapsed 614.20 sec\n",
            "epoch 10, iter 3270, avg. loss 0.12, cum. examples 8621, speed 180.36 examples/sec, time elapsed 615.97 sec\n",
            "epoch 10, iter 3280, avg. loss 0.12, cum. examples 8941, speed 179.70 examples/sec, time elapsed 617.75 sec\n",
            "epoch 10, iter 3290, avg. loss 0.09, cum. examples 9261, speed 183.47 examples/sec, time elapsed 619.50 sec\n",
            "epoch 10, iter 3300, avg. loss 0.13, cum. examples 9581, speed 177.44 examples/sec, time elapsed 621.30 sec\n",
            "epoch 10, iter 3310, avg. loss 0.13, cum. examples 9901, speed 174.00 examples/sec, time elapsed 623.14 sec\n",
            "epoch 10, iter 3320, avg. loss 0.14, cum. examples 10221, speed 177.04 examples/sec, time elapsed 624.95 sec\n",
            "epoch 10, iter 3330, avg. loss 0.09, cum. examples 10541, speed 178.33 examples/sec, time elapsed 626.74 sec\n",
            "epoch 10, iter 3340, avg. loss 0.10, cum. examples 10861, speed 178.53 examples/sec, time elapsed 628.54 sec\n",
            "epoch 10, iter 3350, avg. loss 0.11, cum. examples 11181, speed 177.32 examples/sec, time elapsed 630.34 sec\n",
            "epoch 10, iter 3360, avg. loss 0.10, cum. examples 11501, speed 175.66 examples/sec, time elapsed 632.16 sec\n",
            "epoch 10, iter 3370, avg. loss 0.15, cum. examples 11821, speed 179.92 examples/sec, time elapsed 633.94 sec\n",
            "epoch 10, iter 3380, avg. loss 0.12, cum. examples 12141, speed 182.04 examples/sec, time elapsed 635.70 sec\n",
            "epoch 10, iter 3390, avg. loss 0.08, cum. examples 12461, speed 175.76 examples/sec, time elapsed 637.52 sec\n",
            "epoch 10, iter 3400, avg. loss 0.19, cum. examples 12781, speed 175.29 examples/sec, time elapsed 639.35 sec\n",
            "epoch 10, iter 3410, avg. loss 0.12, cum. examples 13101, speed 174.13 examples/sec, time elapsed 641.18 sec\n",
            "epoch 10, iter 3420, avg. loss 0.14, cum. examples 13421, speed 178.83 examples/sec, time elapsed 642.97 sec\n",
            "epoch 10, iter 3430, avg. loss 0.10, cum. examples 13741, speed 174.54 examples/sec, time elapsed 644.81 sec\n",
            "epoch 10, iter 3440, avg. loss 0.11, cum. examples 14061, speed 176.93 examples/sec, time elapsed 646.62 sec\n",
            "epoch 10, iter 3450, avg. loss 0.08, cum. examples 14381, speed 183.48 examples/sec, time elapsed 648.36 sec\n",
            "epoch 10, iter 3460, avg. loss 0.18, cum. examples 14701, speed 176.36 examples/sec, time elapsed 650.18 sec\n",
            "epoch 10, iter 3470, avg. loss 0.16, cum. examples 15021, speed 176.58 examples/sec, time elapsed 651.99 sec\n",
            "epoch 10, iter 3480, avg. loss 0.12, cum. examples 15341, speed 178.99 examples/sec, time elapsed 653.78 sec\n",
            "epoch 10, iter 3490, avg. loss 0.10, cum. examples 15661, speed 173.46 examples/sec, time elapsed 655.62 sec\n",
            "epoch 10, iter 3500, avg. loss 0.13, cum. examples 15962, speed 181.65 examples/sec, time elapsed 657.28 sec\n",
            "epoch 10, iter 3500, cum. loss 0.13, cum. examples 15962\n",
            "begin validation ...\n",
            "validation: iter 3500, loss 0.842518\n",
            "hit patience 3\n",
            "hit #2 trial\n",
            "load previously best model and decay learning rate to 50.000000%\n",
            "restore parameters of the optimizers\n",
            "epoch 11, iter 3510, avg. loss 0.39, cum. examples 320, speed 42.50 examples/sec, time elapsed 664.81 sec\n",
            "epoch 11, iter 3520, avg. loss 0.35, cum. examples 640, speed 177.47 examples/sec, time elapsed 666.61 sec\n",
            "epoch 11, iter 3530, avg. loss 0.33, cum. examples 960, speed 174.92 examples/sec, time elapsed 668.44 sec\n",
            "epoch 11, iter 3540, avg. loss 0.31, cum. examples 1280, speed 180.94 examples/sec, time elapsed 670.21 sec\n",
            "epoch 11, iter 3550, avg. loss 0.36, cum. examples 1600, speed 175.85 examples/sec, time elapsed 672.03 sec\n",
            "epoch 11, iter 3560, avg. loss 0.24, cum. examples 1920, speed 181.69 examples/sec, time elapsed 673.79 sec\n",
            "epoch 11, iter 3570, avg. loss 0.30, cum. examples 2240, speed 177.05 examples/sec, time elapsed 675.60 sec\n",
            "epoch 11, iter 3580, avg. loss 0.38, cum. examples 2560, speed 178.20 examples/sec, time elapsed 677.40 sec\n",
            "epoch 11, iter 3590, avg. loss 0.38, cum. examples 2880, speed 178.16 examples/sec, time elapsed 679.19 sec\n",
            "epoch 11, iter 3600, avg. loss 0.40, cum. examples 3200, speed 174.58 examples/sec, time elapsed 681.03 sec\n",
            "epoch 11, iter 3610, avg. loss 0.38, cum. examples 3520, speed 175.63 examples/sec, time elapsed 682.85 sec\n",
            "epoch 11, iter 3620, avg. loss 0.38, cum. examples 3840, speed 181.83 examples/sec, time elapsed 684.61 sec\n",
            "epoch 11, iter 3630, avg. loss 0.31, cum. examples 4160, speed 176.49 examples/sec, time elapsed 686.42 sec\n",
            "epoch 11, iter 3640, avg. loss 0.33, cum. examples 4480, speed 178.60 examples/sec, time elapsed 688.21 sec\n",
            "epoch 11, iter 3650, avg. loss 0.30, cum. examples 4800, speed 175.54 examples/sec, time elapsed 690.04 sec\n",
            "epoch 11, iter 3660, avg. loss 0.28, cum. examples 5120, speed 177.28 examples/sec, time elapsed 691.84 sec\n",
            "epoch 11, iter 3670, avg. loss 0.30, cum. examples 5440, speed 182.20 examples/sec, time elapsed 693.60 sec\n",
            "epoch 11, iter 3680, avg. loss 0.37, cum. examples 5760, speed 184.75 examples/sec, time elapsed 695.33 sec\n",
            "epoch 11, iter 3690, avg. loss 0.33, cum. examples 6080, speed 175.41 examples/sec, time elapsed 697.16 sec\n",
            "epoch 11, iter 3700, avg. loss 0.39, cum. examples 6400, speed 174.49 examples/sec, time elapsed 698.99 sec\n",
            "epoch 11, iter 3710, avg. loss 0.36, cum. examples 6720, speed 172.13 examples/sec, time elapsed 700.85 sec\n",
            "epoch 11, iter 3720, avg. loss 0.32, cum. examples 7040, speed 171.93 examples/sec, time elapsed 702.71 sec\n",
            "epoch 11, iter 3730, avg. loss 0.32, cum. examples 7360, speed 176.67 examples/sec, time elapsed 704.52 sec\n",
            "epoch 11, iter 3740, avg. loss 0.36, cum. examples 7680, speed 178.21 examples/sec, time elapsed 706.32 sec\n",
            "epoch 11, iter 3750, avg. loss 0.40, cum. examples 8000, speed 181.07 examples/sec, time elapsed 708.09 sec\n",
            "epoch 11, iter 3760, avg. loss 0.37, cum. examples 8320, speed 175.61 examples/sec, time elapsed 709.91 sec\n",
            "epoch 11, iter 3770, avg. loss 0.37, cum. examples 8640, speed 177.27 examples/sec, time elapsed 711.72 sec\n",
            "epoch 11, iter 3780, avg. loss 0.34, cum. examples 8960, speed 179.46 examples/sec, time elapsed 713.50 sec\n",
            "epoch 11, iter 3790, avg. loss 0.25, cum. examples 9280, speed 182.21 examples/sec, time elapsed 715.26 sec\n",
            "epoch 11, iter 3800, avg. loss 0.37, cum. examples 9600, speed 173.23 examples/sec, time elapsed 717.10 sec\n",
            "epoch 11, iter 3810, avg. loss 0.36, cum. examples 9920, speed 179.69 examples/sec, time elapsed 718.88 sec\n",
            "epoch 11, iter 3820, avg. loss 0.26, cum. examples 10240, speed 178.23 examples/sec, time elapsed 720.68 sec\n",
            "epoch 11, iter 3830, avg. loss 0.33, cum. examples 10560, speed 178.09 examples/sec, time elapsed 722.48 sec\n",
            "epoch 11, iter 3840, avg. loss 0.32, cum. examples 10880, speed 178.62 examples/sec, time elapsed 724.27 sec\n",
            "epoch 11, iter 3850, avg. loss 0.48, cum. examples 11181, speed 168.66 examples/sec, time elapsed 726.05 sec\n",
            "epoch 12, iter 3860, avg. loss 0.25, cum. examples 11501, speed 180.65 examples/sec, time elapsed 727.83 sec\n",
            "epoch 12, iter 3870, avg. loss 0.27, cum. examples 11821, speed 178.82 examples/sec, time elapsed 729.62 sec\n",
            "epoch 12, iter 3880, avg. loss 0.15, cum. examples 12141, speed 176.59 examples/sec, time elapsed 731.43 sec\n",
            "epoch 12, iter 3890, avg. loss 0.24, cum. examples 12461, speed 173.03 examples/sec, time elapsed 733.28 sec\n",
            "epoch 12, iter 3900, avg. loss 0.20, cum. examples 12781, speed 176.62 examples/sec, time elapsed 735.09 sec\n",
            "epoch 12, iter 3910, avg. loss 0.22, cum. examples 13101, speed 179.16 examples/sec, time elapsed 736.88 sec\n",
            "epoch 12, iter 3920, avg. loss 0.26, cum. examples 13421, speed 181.83 examples/sec, time elapsed 738.64 sec\n",
            "epoch 12, iter 3930, avg. loss 0.24, cum. examples 13741, speed 180.56 examples/sec, time elapsed 740.41 sec\n",
            "epoch 12, iter 3940, avg. loss 0.25, cum. examples 14061, speed 175.85 examples/sec, time elapsed 742.23 sec\n",
            "epoch 12, iter 3950, avg. loss 0.26, cum. examples 14381, speed 177.95 examples/sec, time elapsed 744.03 sec\n",
            "epoch 12, iter 3960, avg. loss 0.25, cum. examples 14701, speed 174.15 examples/sec, time elapsed 745.86 sec\n",
            "epoch 12, iter 3970, avg. loss 0.23, cum. examples 15021, speed 176.47 examples/sec, time elapsed 747.68 sec\n",
            "epoch 12, iter 3980, avg. loss 0.22, cum. examples 15341, speed 176.19 examples/sec, time elapsed 749.49 sec\n",
            "epoch 12, iter 3990, avg. loss 0.22, cum. examples 15661, speed 175.93 examples/sec, time elapsed 751.31 sec\n",
            "epoch 12, iter 4000, avg. loss 0.23, cum. examples 15981, speed 178.61 examples/sec, time elapsed 753.11 sec\n",
            "epoch 12, iter 4000, cum. loss 0.31, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 4000, loss 0.622694\n",
            "hit patience 1\n",
            "epoch 12, iter 4010, avg. loss 0.26, cum. examples 320, speed 58.03 examples/sec, time elapsed 758.62 sec\n",
            "epoch 12, iter 4020, avg. loss 0.27, cum. examples 640, speed 174.96 examples/sec, time elapsed 760.45 sec\n",
            "epoch 12, iter 4030, avg. loss 0.25, cum. examples 960, speed 174.18 examples/sec, time elapsed 762.29 sec\n",
            "epoch 12, iter 4040, avg. loss 0.28, cum. examples 1280, speed 176.74 examples/sec, time elapsed 764.10 sec\n",
            "epoch 12, iter 4050, avg. loss 0.23, cum. examples 1600, speed 174.16 examples/sec, time elapsed 765.94 sec\n",
            "epoch 12, iter 4060, avg. loss 0.15, cum. examples 1920, speed 181.33 examples/sec, time elapsed 767.70 sec\n",
            "epoch 12, iter 4070, avg. loss 0.30, cum. examples 2240, speed 178.87 examples/sec, time elapsed 769.49 sec\n",
            "epoch 12, iter 4080, avg. loss 0.35, cum. examples 2560, speed 178.50 examples/sec, time elapsed 771.28 sec\n",
            "epoch 12, iter 4090, avg. loss 0.26, cum. examples 2880, speed 178.09 examples/sec, time elapsed 773.08 sec\n",
            "epoch 12, iter 4100, avg. loss 0.25, cum. examples 3200, speed 174.66 examples/sec, time elapsed 774.91 sec\n",
            "epoch 12, iter 4110, avg. loss 0.27, cum. examples 3520, speed 175.51 examples/sec, time elapsed 776.73 sec\n",
            "epoch 12, iter 4120, avg. loss 0.26, cum. examples 3840, speed 175.02 examples/sec, time elapsed 778.56 sec\n",
            "epoch 12, iter 4130, avg. loss 0.34, cum. examples 4160, speed 178.87 examples/sec, time elapsed 780.35 sec\n",
            "epoch 12, iter 4140, avg. loss 0.23, cum. examples 4480, speed 179.54 examples/sec, time elapsed 782.13 sec\n",
            "epoch 12, iter 4150, avg. loss 0.23, cum. examples 4800, speed 177.23 examples/sec, time elapsed 783.94 sec\n",
            "epoch 12, iter 4160, avg. loss 0.18, cum. examples 5120, speed 180.84 examples/sec, time elapsed 785.71 sec\n",
            "epoch 12, iter 4170, avg. loss 0.24, cum. examples 5440, speed 179.94 examples/sec, time elapsed 787.49 sec\n",
            "epoch 12, iter 4180, avg. loss 0.24, cum. examples 5760, speed 179.75 examples/sec, time elapsed 789.27 sec\n",
            "epoch 12, iter 4190, avg. loss 0.36, cum. examples 6080, speed 180.39 examples/sec, time elapsed 791.04 sec\n",
            "epoch 12, iter 4200, avg. loss 0.25, cum. examples 6381, speed 176.80 examples/sec, time elapsed 792.75 sec\n",
            "epoch 13, iter 4210, avg. loss 0.18, cum. examples 6701, speed 175.40 examples/sec, time elapsed 794.57 sec\n",
            "epoch 13, iter 4220, avg. loss 0.19, cum. examples 7021, speed 175.33 examples/sec, time elapsed 796.40 sec\n",
            "epoch 13, iter 4230, avg. loss 0.13, cum. examples 7341, speed 179.42 examples/sec, time elapsed 798.18 sec\n",
            "epoch 13, iter 4240, avg. loss 0.15, cum. examples 7661, speed 179.39 examples/sec, time elapsed 799.96 sec\n",
            "epoch 13, iter 4250, avg. loss 0.18, cum. examples 7981, speed 176.29 examples/sec, time elapsed 801.78 sec\n",
            "epoch 13, iter 4260, avg. loss 0.17, cum. examples 8301, speed 174.86 examples/sec, time elapsed 803.61 sec\n",
            "epoch 13, iter 4270, avg. loss 0.11, cum. examples 8621, speed 177.27 examples/sec, time elapsed 805.42 sec\n",
            "epoch 13, iter 4280, avg. loss 0.15, cum. examples 8941, speed 183.38 examples/sec, time elapsed 807.16 sec\n",
            "epoch 13, iter 4290, avg. loss 0.14, cum. examples 9261, speed 180.07 examples/sec, time elapsed 808.94 sec\n",
            "epoch 13, iter 4300, avg. loss 0.24, cum. examples 9581, speed 182.99 examples/sec, time elapsed 810.69 sec\n",
            "epoch 13, iter 4310, avg. loss 0.11, cum. examples 9901, speed 183.18 examples/sec, time elapsed 812.44 sec\n",
            "epoch 13, iter 4320, avg. loss 0.14, cum. examples 10221, speed 180.53 examples/sec, time elapsed 814.21 sec\n",
            "epoch 13, iter 4330, avg. loss 0.18, cum. examples 10541, speed 185.07 examples/sec, time elapsed 815.94 sec\n",
            "epoch 13, iter 4340, avg. loss 0.11, cum. examples 10861, speed 172.13 examples/sec, time elapsed 817.80 sec\n",
            "epoch 13, iter 4350, avg. loss 0.13, cum. examples 11181, speed 183.03 examples/sec, time elapsed 819.55 sec\n",
            "epoch 13, iter 4360, avg. loss 0.13, cum. examples 11501, speed 184.67 examples/sec, time elapsed 821.28 sec\n",
            "epoch 13, iter 4370, avg. loss 0.17, cum. examples 11821, speed 182.77 examples/sec, time elapsed 823.03 sec\n",
            "epoch 13, iter 4380, avg. loss 0.10, cum. examples 12141, speed 170.15 examples/sec, time elapsed 824.91 sec\n",
            "epoch 13, iter 4390, avg. loss 0.19, cum. examples 12461, speed 175.50 examples/sec, time elapsed 826.74 sec\n",
            "epoch 13, iter 4400, avg. loss 0.18, cum. examples 12781, speed 178.98 examples/sec, time elapsed 828.52 sec\n",
            "epoch 13, iter 4410, avg. loss 0.15, cum. examples 13101, speed 181.24 examples/sec, time elapsed 830.29 sec\n",
            "epoch 13, iter 4420, avg. loss 0.25, cum. examples 13421, speed 173.39 examples/sec, time elapsed 832.14 sec\n",
            "epoch 13, iter 4430, avg. loss 0.16, cum. examples 13741, speed 175.12 examples/sec, time elapsed 833.96 sec\n",
            "epoch 13, iter 4440, avg. loss 0.19, cum. examples 14061, speed 172.98 examples/sec, time elapsed 835.81 sec\n",
            "epoch 13, iter 4450, avg. loss 0.12, cum. examples 14381, speed 180.20 examples/sec, time elapsed 837.59 sec\n",
            "epoch 13, iter 4460, avg. loss 0.23, cum. examples 14701, speed 179.09 examples/sec, time elapsed 839.38 sec\n",
            "epoch 13, iter 4470, avg. loss 0.21, cum. examples 15021, speed 171.86 examples/sec, time elapsed 841.24 sec\n",
            "epoch 13, iter 4480, avg. loss 0.30, cum. examples 15341, speed 177.83 examples/sec, time elapsed 843.04 sec\n",
            "epoch 13, iter 4490, avg. loss 0.18, cum. examples 15661, speed 177.28 examples/sec, time elapsed 844.85 sec\n",
            "epoch 13, iter 4500, avg. loss 0.22, cum. examples 15981, speed 179.25 examples/sec, time elapsed 846.63 sec\n",
            "epoch 13, iter 4500, cum. loss 0.21, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 4500, loss 0.745790\n",
            "hit patience 2\n",
            "epoch 13, iter 4510, avg. loss 0.20, cum. examples 320, speed 57.91 examples/sec, time elapsed 852.16 sec\n",
            "epoch 13, iter 4520, avg. loss 0.16, cum. examples 640, speed 173.24 examples/sec, time elapsed 854.00 sec\n",
            "epoch 13, iter 4530, avg. loss 0.19, cum. examples 960, speed 176.36 examples/sec, time elapsed 855.82 sec\n",
            "epoch 13, iter 4540, avg. loss 0.18, cum. examples 1280, speed 180.50 examples/sec, time elapsed 857.59 sec\n",
            "epoch 13, iter 4550, avg. loss 0.22, cum. examples 1581, speed 176.47 examples/sec, time elapsed 859.30 sec\n",
            "epoch 14, iter 4560, avg. loss 0.12, cum. examples 1901, speed 186.29 examples/sec, time elapsed 861.02 sec\n",
            "epoch 14, iter 4570, avg. loss 0.12, cum. examples 2221, speed 176.83 examples/sec, time elapsed 862.83 sec\n",
            "epoch 14, iter 4580, avg. loss 0.12, cum. examples 2541, speed 179.62 examples/sec, time elapsed 864.61 sec\n",
            "epoch 14, iter 4590, avg. loss 0.11, cum. examples 2861, speed 178.80 examples/sec, time elapsed 866.40 sec\n",
            "epoch 14, iter 4600, avg. loss 0.07, cum. examples 3181, speed 178.34 examples/sec, time elapsed 868.19 sec\n",
            "epoch 14, iter 4610, avg. loss 0.13, cum. examples 3501, speed 179.77 examples/sec, time elapsed 869.97 sec\n",
            "epoch 14, iter 4620, avg. loss 0.09, cum. examples 3821, speed 177.73 examples/sec, time elapsed 871.77 sec\n",
            "epoch 14, iter 4630, avg. loss 0.10, cum. examples 4141, speed 178.23 examples/sec, time elapsed 873.57 sec\n",
            "epoch 14, iter 4640, avg. loss 0.14, cum. examples 4461, speed 183.62 examples/sec, time elapsed 875.31 sec\n",
            "epoch 14, iter 4650, avg. loss 0.11, cum. examples 4781, speed 178.72 examples/sec, time elapsed 877.10 sec\n",
            "epoch 14, iter 4660, avg. loss 0.08, cum. examples 5101, speed 178.53 examples/sec, time elapsed 878.89 sec\n",
            "epoch 14, iter 4670, avg. loss 0.18, cum. examples 5421, speed 177.78 examples/sec, time elapsed 880.69 sec\n",
            "epoch 14, iter 4680, avg. loss 0.14, cum. examples 5741, speed 174.86 examples/sec, time elapsed 882.52 sec\n",
            "epoch 14, iter 4690, avg. loss 0.08, cum. examples 6061, speed 183.58 examples/sec, time elapsed 884.27 sec\n",
            "epoch 14, iter 4700, avg. loss 0.13, cum. examples 6381, speed 177.98 examples/sec, time elapsed 886.07 sec\n",
            "epoch 14, iter 4710, avg. loss 0.10, cum. examples 6701, speed 175.75 examples/sec, time elapsed 887.89 sec\n",
            "epoch 14, iter 4720, avg. loss 0.15, cum. examples 7021, speed 177.20 examples/sec, time elapsed 889.69 sec\n",
            "epoch 14, iter 4730, avg. loss 0.14, cum. examples 7341, speed 175.58 examples/sec, time elapsed 891.52 sec\n",
            "epoch 14, iter 4740, avg. loss 0.15, cum. examples 7661, speed 177.43 examples/sec, time elapsed 893.32 sec\n",
            "epoch 14, iter 4750, avg. loss 0.13, cum. examples 7981, speed 178.85 examples/sec, time elapsed 895.11 sec\n",
            "epoch 14, iter 4760, avg. loss 0.12, cum. examples 8301, speed 172.30 examples/sec, time elapsed 896.97 sec\n",
            "epoch 14, iter 4770, avg. loss 0.16, cum. examples 8621, speed 179.23 examples/sec, time elapsed 898.75 sec\n",
            "epoch 14, iter 4780, avg. loss 0.10, cum. examples 8941, speed 170.34 examples/sec, time elapsed 900.63 sec\n",
            "epoch 14, iter 4790, avg. loss 0.13, cum. examples 9261, speed 174.77 examples/sec, time elapsed 902.46 sec\n",
            "epoch 14, iter 4800, avg. loss 0.08, cum. examples 9581, speed 180.10 examples/sec, time elapsed 904.24 sec\n",
            "epoch 14, iter 4810, avg. loss 0.10, cum. examples 9901, speed 174.91 examples/sec, time elapsed 906.07 sec\n",
            "epoch 14, iter 4820, avg. loss 0.15, cum. examples 10221, speed 177.48 examples/sec, time elapsed 907.87 sec\n",
            "epoch 14, iter 4830, avg. loss 0.16, cum. examples 10541, speed 179.35 examples/sec, time elapsed 909.66 sec\n",
            "epoch 14, iter 4840, avg. loss 0.11, cum. examples 10861, speed 174.33 examples/sec, time elapsed 911.49 sec\n",
            "epoch 14, iter 4850, avg. loss 0.13, cum. examples 11181, speed 172.20 examples/sec, time elapsed 913.35 sec\n",
            "epoch 14, iter 4860, avg. loss 0.08, cum. examples 11501, speed 180.89 examples/sec, time elapsed 915.12 sec\n",
            "epoch 14, iter 4870, avg. loss 0.17, cum. examples 11821, speed 178.85 examples/sec, time elapsed 916.91 sec\n",
            "epoch 14, iter 4880, avg. loss 0.11, cum. examples 12141, speed 174.98 examples/sec, time elapsed 918.74 sec\n",
            "epoch 14, iter 4890, avg. loss 0.12, cum. examples 12461, speed 179.12 examples/sec, time elapsed 920.53 sec\n",
            "epoch 14, iter 4900, avg. loss 0.20, cum. examples 12762, speed 176.38 examples/sec, time elapsed 922.23 sec\n",
            "epoch 15, iter 4910, avg. loss 0.10, cum. examples 13082, speed 178.90 examples/sec, time elapsed 924.02 sec\n",
            "epoch 15, iter 4920, avg. loss 0.09, cum. examples 13402, speed 178.06 examples/sec, time elapsed 925.82 sec\n",
            "epoch 15, iter 4930, avg. loss 0.10, cum. examples 13722, speed 176.46 examples/sec, time elapsed 927.63 sec\n",
            "epoch 15, iter 4940, avg. loss 0.06, cum. examples 14042, speed 175.83 examples/sec, time elapsed 929.45 sec\n",
            "epoch 15, iter 4950, avg. loss 0.09, cum. examples 14362, speed 181.30 examples/sec, time elapsed 931.22 sec\n",
            "epoch 15, iter 4960, avg. loss 0.09, cum. examples 14682, speed 176.14 examples/sec, time elapsed 933.03 sec\n",
            "epoch 15, iter 4970, avg. loss 0.08, cum. examples 15002, speed 175.15 examples/sec, time elapsed 934.86 sec\n",
            "epoch 15, iter 4980, avg. loss 0.07, cum. examples 15322, speed 178.07 examples/sec, time elapsed 936.66 sec\n",
            "epoch 15, iter 4990, avg. loss 0.06, cum. examples 15642, speed 179.25 examples/sec, time elapsed 938.44 sec\n",
            "epoch 15, iter 5000, avg. loss 0.05, cum. examples 15962, speed 181.67 examples/sec, time elapsed 940.21 sec\n",
            "epoch 15, iter 5000, cum. loss 0.12, cum. examples 15962\n",
            "begin validation ...\n",
            "validation: iter 5000, loss 1.034668\n",
            "hit patience 3\n",
            "hit #3 trial\n",
            "early stop!\n",
            "load previously best model and decay learning rate to 50.000000%\n",
            "restore parameters of the optimizers\n",
            "epoch 15, iter 5010, avg. loss 0.26, cum. examples 320, speed 42.51 examples/sec, time elapsed 947.73 sec\n",
            "epoch 15, iter 5020, avg. loss 0.39, cum. examples 640, speed 176.88 examples/sec, time elapsed 949.54 sec\n",
            "epoch 15, iter 5030, avg. loss 0.39, cum. examples 960, speed 180.40 examples/sec, time elapsed 951.32 sec\n",
            "epoch 15, iter 5040, avg. loss 0.44, cum. examples 1280, speed 174.97 examples/sec, time elapsed 953.15 sec\n",
            "epoch 15, iter 5050, avg. loss 0.39, cum. examples 1600, speed 170.90 examples/sec, time elapsed 955.02 sec\n",
            "epoch 15, iter 5060, avg. loss 0.35, cum. examples 1920, speed 178.32 examples/sec, time elapsed 956.82 sec\n",
            "epoch 15, iter 5070, avg. loss 0.29, cum. examples 2240, speed 179.06 examples/sec, time elapsed 958.60 sec\n",
            "epoch 15, iter 5080, avg. loss 0.32, cum. examples 2560, speed 177.05 examples/sec, time elapsed 960.41 sec\n",
            "epoch 15, iter 5090, avg. loss 0.30, cum. examples 2880, speed 178.78 examples/sec, time elapsed 962.20 sec\n",
            "epoch 15, iter 5100, avg. loss 0.30, cum. examples 3200, speed 178.72 examples/sec, time elapsed 963.99 sec\n",
            "epoch 15, iter 5110, avg. loss 0.43, cum. examples 3520, speed 176.88 examples/sec, time elapsed 965.80 sec\n",
            "epoch 15, iter 5120, avg. loss 0.49, cum. examples 3840, speed 180.02 examples/sec, time elapsed 967.58 sec\n",
            "epoch 15, iter 5130, avg. loss 0.33, cum. examples 4160, speed 182.57 examples/sec, time elapsed 969.33 sec\n",
            "epoch 15, iter 5140, avg. loss 0.34, cum. examples 4480, speed 175.28 examples/sec, time elapsed 971.16 sec\n",
            "epoch 15, iter 5150, avg. loss 0.33, cum. examples 4800, speed 180.30 examples/sec, time elapsed 972.93 sec\n",
            "epoch 15, iter 5160, avg. loss 0.33, cum. examples 5120, speed 182.63 examples/sec, time elapsed 974.69 sec\n",
            "epoch 15, iter 5170, avg. loss 0.41, cum. examples 5440, speed 177.21 examples/sec, time elapsed 976.49 sec\n",
            "epoch 15, iter 5180, avg. loss 0.28, cum. examples 5760, speed 181.85 examples/sec, time elapsed 978.25 sec\n",
            "epoch 15, iter 5190, avg. loss 0.23, cum. examples 6080, speed 185.08 examples/sec, time elapsed 979.98 sec\n",
            "epoch 15, iter 5200, avg. loss 0.30, cum. examples 6400, speed 179.55 examples/sec, time elapsed 981.76 sec\n",
            "epoch 15, iter 5210, avg. loss 0.34, cum. examples 6720, speed 176.02 examples/sec, time elapsed 983.58 sec\n",
            "epoch 15, iter 5220, avg. loss 0.45, cum. examples 7040, speed 180.96 examples/sec, time elapsed 985.35 sec\n",
            "epoch 15, iter 5230, avg. loss 0.31, cum. examples 7360, speed 179.42 examples/sec, time elapsed 987.13 sec\n",
            "epoch 15, iter 5240, avg. loss 0.31, cum. examples 7680, speed 175.42 examples/sec, time elapsed 988.96 sec\n",
            "epoch 15, iter 5250, avg. loss 0.27, cum. examples 7981, speed 172.26 examples/sec, time elapsed 990.71 sec\n",
            "epoch 16, iter 5260, avg. loss 0.25, cum. examples 8301, speed 180.93 examples/sec, time elapsed 992.48 sec\n",
            "epoch 16, iter 5270, avg. loss 0.31, cum. examples 8621, speed 183.64 examples/sec, time elapsed 994.22 sec\n",
            "epoch 16, iter 5280, avg. loss 0.23, cum. examples 8941, speed 181.06 examples/sec, time elapsed 995.99 sec\n",
            "epoch 16, iter 5290, avg. loss 0.26, cum. examples 9261, speed 177.03 examples/sec, time elapsed 997.79 sec\n",
            "epoch 16, iter 5300, avg. loss 0.32, cum. examples 9581, speed 178.80 examples/sec, time elapsed 999.58 sec\n",
            "epoch 16, iter 5310, avg. loss 0.27, cum. examples 9901, speed 174.34 examples/sec, time elapsed 1001.42 sec\n",
            "epoch 16, iter 5320, avg. loss 0.20, cum. examples 10221, speed 183.17 examples/sec, time elapsed 1003.17 sec\n",
            "epoch 16, iter 5330, avg. loss 0.27, cum. examples 10541, speed 176.58 examples/sec, time elapsed 1004.98 sec\n",
            "epoch 16, iter 5340, avg. loss 0.26, cum. examples 10861, speed 169.12 examples/sec, time elapsed 1006.87 sec\n",
            "epoch 16, iter 5350, avg. loss 0.29, cum. examples 11181, speed 178.43 examples/sec, time elapsed 1008.67 sec\n",
            "epoch 16, iter 5360, avg. loss 0.33, cum. examples 11501, speed 177.78 examples/sec, time elapsed 1010.47 sec\n",
            "epoch 16, iter 5370, avg. loss 0.34, cum. examples 11821, speed 172.87 examples/sec, time elapsed 1012.32 sec\n",
            "epoch 16, iter 5380, avg. loss 0.25, cum. examples 12141, speed 176.41 examples/sec, time elapsed 1014.13 sec\n",
            "epoch 16, iter 5390, avg. loss 0.36, cum. examples 12461, speed 177.29 examples/sec, time elapsed 1015.94 sec\n",
            "epoch 16, iter 5400, avg. loss 0.23, cum. examples 12781, speed 177.86 examples/sec, time elapsed 1017.74 sec\n",
            "epoch 16, iter 5410, avg. loss 0.20, cum. examples 13101, speed 179.85 examples/sec, time elapsed 1019.52 sec\n",
            "epoch 16, iter 5420, avg. loss 0.29, cum. examples 13421, speed 177.00 examples/sec, time elapsed 1021.32 sec\n",
            "epoch 16, iter 5430, avg. loss 0.24, cum. examples 13741, speed 175.04 examples/sec, time elapsed 1023.15 sec\n",
            "epoch 16, iter 5440, avg. loss 0.22, cum. examples 14061, speed 177.28 examples/sec, time elapsed 1024.96 sec\n",
            "epoch 16, iter 5450, avg. loss 0.21, cum. examples 14381, speed 172.92 examples/sec, time elapsed 1026.81 sec\n",
            "epoch 16, iter 5460, avg. loss 0.26, cum. examples 14701, speed 180.46 examples/sec, time elapsed 1028.58 sec\n",
            "epoch 16, iter 5470, avg. loss 0.32, cum. examples 15021, speed 173.46 examples/sec, time elapsed 1030.43 sec\n",
            "epoch 16, iter 5480, avg. loss 0.27, cum. examples 15341, speed 179.94 examples/sec, time elapsed 1032.21 sec\n",
            "epoch 16, iter 5490, avg. loss 0.40, cum. examples 15661, speed 172.66 examples/sec, time elapsed 1034.06 sec\n",
            "epoch 16, iter 5500, avg. loss 0.24, cum. examples 15981, speed 179.71 examples/sec, time elapsed 1035.84 sec\n",
            "epoch 16, iter 5500, cum. loss 0.31, cum. examples 15981\n",
            "begin validation ...\n",
            "validation: iter 5500, loss 0.558972\n",
            "hit patience 1\n",
            "epoch 16, iter 5510, avg. loss 0.30, cum. examples 320, speed 57.65 examples/sec, time elapsed 1041.39 sec\n",
            "epoch 16, iter 5520, avg. loss 0.26, cum. examples 640, speed 176.78 examples/sec, time elapsed 1043.20 sec\n",
            "epoch 16, iter 5530, avg. loss 0.33, cum. examples 960, speed 180.43 examples/sec, time elapsed 1044.98 sec\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-40a22b790974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpre_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Calculate loss and gradient function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-33ad492e9447>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sents)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msents_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msents_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpre_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msents_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpre_softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m                             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                             inputs_embeds=inputs_embeds)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    738\u001b[0m                                        \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                                        \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                                        encoder_attention_mask=encoder_extended_attention_mask)\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mmixed_key_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LytvxJNbtIxT",
        "colab_type": "text"
      },
      "source": [
        "## **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzul1PqOvO6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    import numpy as np\n",
        "    import pickle\n",
        "    from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, \\\n",
        "    f1_score, precision_score, recall_score, roc_auc_score\n",
        "    import matplotlib\n",
        "    matplotlib.use('agg')\n",
        "    from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph2MIvLovLiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, path='cm', cmap=plt.cm.Reds):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    pickle.dump(cm, open(path, 'wb'))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNr2YlyIMLWr",
        "colab_type": "code",
        "outputId": "c68bc675-6ea3-495a-9e29-07b933a5f180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "    print('load best model...')\n",
        "\n",
        "    model = SentimentClassifierModel.load('/content/gdrive/My Drive/Colab Notebooks/' + prefix + '_model.bin', device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    df_test = validation_data\n",
        "\n",
        "    df_test = df_test.sort_values(by='BERT_processed_text_length', ascending=False)\n",
        "\n",
        "    test_batch_size = 32\n",
        "\n",
        "    n_batch = int(np.ceil(df_test.shape[0]/test_batch_size))\n",
        "\n",
        "    cn_loss = torch.load('loss_func', map_location=lambda storage, loc: storage).to(device)\n",
        "\n",
        "    ProcessedText_BERT = list(df_test.BERT_processed_text)\n",
        "    InformationType_label = list(df_test.airline_sentiment_label)\n",
        "\n",
        "    test_loss = 0.\n",
        "    prediction = []\n",
        "    prob = []\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batch):\n",
        "            sents = ProcessedText_BERT[i*test_batch_size: (i+1)*test_batch_size]\n",
        "            targets = torch.tensor(InformationType_label[i * test_batch_size: (i + 1) * test_batch_size],\n",
        "                                   dtype=torch.long, device=device)\n",
        "            batch_size = len(sents)\n",
        "\n",
        "            pre_softmax = model(sents)[0]\n",
        "            batch_loss = cn_loss(pre_softmax, targets)\n",
        "            test_loss += batch_loss.item()*batch_size\n",
        "            prob_batch = softmax(pre_softmax)\n",
        "            prob.append(prob_batch)\n",
        "\n",
        "            prediction.extend([t.item() for t in list(torch.argmax(prob_batch, dim=1))])\n",
        "\n",
        "    prob = torch.cat(tuple(prob), dim=0)\n",
        "    loss = test_loss/df_test.shape[0]\n",
        "\n",
        "    pickle.dump([label_names[i] for i in prediction], open(prefix+'_test_prediction', 'wb'))\n",
        "    pickle.dump(prob.data.cpu().numpy(), open(prefix + '_test_prediction_prob', 'wb'))\n",
        "\n",
        "    accuracy = accuracy_score(df_test.airline_sentiment_label.values, prediction)\n",
        "    matthews = matthews_corrcoef(df_test.airline_sentiment_label.values, prediction)\n",
        "\n",
        "    precisions = {}\n",
        "    recalls = {}\n",
        "    f1s = {}\n",
        "    aucrocs = {}\n",
        "\n",
        "    for i in range(len(label_names)):\n",
        "        prediction_ = [1 if pred == i else 0 for pred in prediction]\n",
        "        true_ = [1 if label == i else 0 for label in df_test.airline_sentiment_label.values]\n",
        "        f1s.update({label_names[i]: f1_score(true_, prediction_)})\n",
        "        precisions.update({label_names[i]: precision_score(true_, prediction_)})\n",
        "        recalls.update({label_names[i]: recall_score(true_, prediction_)})\n",
        "        aucrocs.update({label_names[i]: roc_auc_score(true_, list(t.item() for t in prob[:, i]))})\n",
        "\n",
        "    metrics_dict = {'loss': loss, 'accuracy': accuracy, 'matthews coef': matthews, 'precision': precisions,\n",
        "                         'recall': recalls, 'f1': f1s, 'aucroc': aucrocs}\n",
        "\n",
        "    pickle.dump(metrics_dict, open(prefix+'_evaluation_metrics', 'wb'))\n",
        "\n",
        "    cm = plot_confusion_matrix(list(df_test.airline_sentiment_label.values), prediction, label_names, normalize=False,\n",
        "                          path=prefix+'_test_confusion_matrix', title='confusion matrix for test dataset')\n",
        "    plt.savefig(prefix+'_test_confusion_matrix', format='png')\n",
        "    cm_norm = plot_confusion_matrix(list(df_test.airline_sentiment_label.values), prediction, label_names, normalize=True,\n",
        "                          path=prefix+'_test normalized_confusion_matrix', title='normalized confusion matrix for test dataset')\n",
        "    plt.savefig(prefix+'_test_normalized_confusion_matrix', format='png')\n",
        "\n",
        "    print('loss: %.2f' % loss)\n",
        "    print('accuracy: %.2f' % accuracy)\n",
        "    print('matthews coef: %.2f' % matthews)\n",
        "    print('-' * 80)\n",
        "    for i in range(len(label_names)):\n",
        "        print('precision score for %s: %.2f' % (label_names[i], precisions[label_names[i]]))\n",
        "        print('recall score for %s: %.2f' % (label_names[i], recalls[label_names[i]]))\n",
        "        print('f1 score for %s: %.2f' % (label_names[i], f1s[label_names[i]]))\n",
        "        print('auc roc score for %s: %.2f' % (label_names[i], aucrocs[label_names[i]]))\n",
        "        print('-' * 80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load best model...\n",
            "loss: 0.55\n",
            "accuracy: 0.83\n",
            "matthews coef: 0.69\n",
            "--------------------------------------------------------------------------------\n",
            "precision score for positive: 0.89\n",
            "recall score for positive: 0.90\n",
            "f1 score for positive: 0.89\n",
            "auc roc score for positive: 0.94\n",
            "--------------------------------------------------------------------------------\n",
            "precision score for negative: 0.73\n",
            "recall score for negative: 0.62\n",
            "f1 score for negative: 0.67\n",
            "auc roc score for negative: 0.90\n",
            "--------------------------------------------------------------------------------\n",
            "precision score for neutral: 0.74\n",
            "recall score for neutral: 0.87\n",
            "f1 score for neutral: 0.80\n",
            "auc roc score for neutral: 0.97\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vmuPRJv3tW",
        "colab_type": "code",
        "outputId": "84d768b2-e631-4c32-e2b3-4529d9255048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "precisions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': 0.7326923076923076,\n",
              " 'neutral': 0.7437858508604207,\n",
              " 'positive': 0.8887621220764403}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRa9nDlIwFMe",
        "colab_type": "code",
        "outputId": "b1212e52-4f83-405c-d825-17224b277064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "recalls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': 0.6195121951219512,\n",
              " 'neutral': 0.8702460850111857,\n",
              " 'positive': 0.8985005767012687}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ4qRQcLwKzJ",
        "colab_type": "code",
        "outputId": "119ad7a1-65e5-4065-ce12-3c951ff49abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "f1s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': 0.6713656387665198,\n",
              " 'neutral': 0.8020618556701031,\n",
              " 'positive': 0.8936048178950387}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppl7EDjowNDE",
        "colab_type": "code",
        "outputId": "371e109e-01e5-4767-c75c-5f8b81212569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "aucrocs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': 0.9045302557564778,\n",
              " 'neutral': 0.9672781887289846,\n",
              " 'positive': 0.9354930850151072}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_si1hG-nkWD",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocess Straits Times comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA_CiKzQanMW",
        "colab_type": "code",
        "outputId": "88703f02-b4de-4a09-80e1-a289537709e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import pandas\n",
        "st_df = pandas.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/st-comments.csv\", index_col=0, encoding='latin-1', usecols=['comment_id','post_title', 'comment_text'])\n",
        "st_df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_title</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comment_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>Better to be happy bringing up your child than...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>..kinda in this boat at the moment , feeling v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>So sad that the marriage institution is taken ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>When a women found a men younger than her, ppl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>Both gained what they want.guy want short happ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   post_title                                       comment_text\n",
              "comment_id                                                                                                      \n",
              "0           Husband runs off when wife is pregnant and def...  Better to be happy bringing up your child than...\n",
              "1           Husband runs off when wife is pregnant and def...  ..kinda in this boat at the moment , feeling v...\n",
              "2           Husband runs off when wife is pregnant and def...  So sad that the marriage institution is taken ...\n",
              "3           Husband runs off when wife is pregnant and def...  When a women found a men younger than her, ppl...\n",
              "4           Husband runs off when wife is pregnant and def...  Both gained what they want.guy want short happ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVuEvinga8iI",
        "colab_type": "code",
        "outputId": "fa6d69a8-9229-46b8-963a-780593440625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove URL, RT, mention(@)\n",
        "\n",
        "st_df['text'] = st_df.comment_text\n",
        "\n",
        "st_df.text = st_df.text.str.replace(r'http(\\S)+', r'')\n",
        "st_df.text = st_df.text.str.replace(r'http ...', r'')\n",
        "st_df.text = st_df.text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
        "st_df.text = st_df.text.str.replace(r'@[\\S]+',r'')\n",
        "\n",
        "# Remove non-ascii words or characters\n",
        "st_df.text = [''.join([i if ord(i) < 128 else '' for i in text]) for text in st_df.text]\n",
        "st_df.text = st_df.text.str.replace(r'_[\\S]?',r'')\n",
        "\n",
        "# Remove extra space\n",
        "st_df.text = st_df.text.str.replace(r'[ ]{2, }',r' ')\n",
        "\n",
        "# Remove &, < and >\n",
        "st_df.text = st_df.text.str.replace(r'&amp;?',r'and')\n",
        "st_df.text = st_df.text.str.replace(r'&lt;',r'<')\n",
        "st_df.text = st_df.text.str.replace(r'&gt;',r'>')\n",
        "\n",
        "# Insert space between words and punctuation marks\n",
        "st_df.text = st_df.text.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
        "st_df.text = st_df.text.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
        "\n",
        "# Lowercased and strip\n",
        "st_df.text = st_df.text.str.lower()\n",
        "st_df.text = st_df.text.str.strip()\n",
        "\n",
        "st_df['text_length'] = [len(text.split(' ')) for text in st_df.text]\n",
        "print(st_df.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(257, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQtufB-4rQnI",
        "colab_type": "text"
      },
      "source": [
        "### **Preprocess comments into BERT format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E_HLiLda_mo",
        "colab_type": "code",
        "outputId": "f0fc3e96-e097-4a3a-9b0f-c24cfe90ea23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "st_df['BERT_processed_text'] = '[CLS] '+ st_df.text\n",
        "st_df.BERT_processed_text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment_id\n",
              "0      [CLS] better to be happy bringing up your chil...\n",
              "1      [CLS] .. kinda in this boat at the moment , fe...\n",
              "2      [CLS] so sad that the marriage institution is ...\n",
              "3      [CLS] when a women found a men younger than he...\n",
              "4      [CLS] both gained what they want . guy want sh...\n",
              "                             ...                        \n",
              "252    [CLS] how come this photographer skill like da...\n",
              "253    [CLS] this tension between perspective of citi...\n",
              "254                          [CLS] send in the eagles !!\n",
              "255    [CLS] sling shot lah . just encourage to shoot...\n",
              "256    [CLS] deploy the air rifle club people to trai...\n",
              "Name: BERT_processed_text, Length: 257, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frz8uvXubQxL",
        "colab_type": "code",
        "outputId": "cb112678-08c3-4812-822a-41335f50a0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "st_df['BERT_processed_text_length'] = [len(tokenizer.tokenize(sent)) for sent in st_df.text]\n",
        "st_df.BERT_processed_text_length"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment_id\n",
              "0      34\n",
              "1      53\n",
              "2      95\n",
              "3      88\n",
              "4      17\n",
              "       ..\n",
              "252    11\n",
              "253    23\n",
              "254     6\n",
              "255    21\n",
              "256    14\n",
              "Name: BERT_processed_text_length, Length: 257, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_of_7FD-r6Dp",
        "colab_type": "code",
        "outputId": "25dfdf5a-e63e-44d7-ca53-d23d8ae46622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "st_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_title</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>BERT_processed_text</th>\n",
              "      <th>BERT_processed_text_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comment_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>Better to be happy bringing up your child than...</td>\n",
              "      <td>better to be happy bringing up your child than...</td>\n",
              "      <td>30</td>\n",
              "      <td>[CLS] better to be happy bringing up your chil...</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>..kinda in this boat at the moment , feeling v...</td>\n",
              "      <td>.. kinda in this boat at the moment , feeling ...</td>\n",
              "      <td>49</td>\n",
              "      <td>[CLS] .. kinda in this boat at the moment , fe...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>So sad that the marriage institution is taken ...</td>\n",
              "      <td>so sad that the marriage institution is taken ...</td>\n",
              "      <td>95</td>\n",
              "      <td>[CLS] so sad that the marriage institution is ...</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>When a women found a men younger than her, ppl...</td>\n",
              "      <td>when a women found a men younger than her , pp...</td>\n",
              "      <td>78</td>\n",
              "      <td>[CLS] when a women found a men younger than he...</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Husband runs off when wife is pregnant and def...</td>\n",
              "      <td>Both gained what they want.guy want short happ...</td>\n",
              "      <td>both gained what they want . guy want short ha...</td>\n",
              "      <td>17</td>\n",
              "      <td>[CLS] both gained what they want . guy want sh...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>How come this photographer skill like dat one ...</td>\n",
              "      <td>how come this photographer skill like dat one ...</td>\n",
              "      <td>9</td>\n",
              "      <td>[CLS] how come this photographer skill like da...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>This tension between perspective of citizens l...</td>\n",
              "      <td>this tension between perspective of citizens l...</td>\n",
              "      <td>22</td>\n",
              "      <td>[CLS] this tension between perspective of citi...</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>Send in the eagles!!</td>\n",
              "      <td>send in the eagles !!</td>\n",
              "      <td>5</td>\n",
              "      <td>[CLS] send in the eagles !!</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>Sling shot lah. Just encourage to shoot them. ...</td>\n",
              "      <td>sling shot lah . just encourage to shoot them ...</td>\n",
              "      <td>20</td>\n",
              "      <td>[CLS] sling shot lah . just encourage to shoot...</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>Deploy the air rifle club people to train for ...</td>\n",
              "      <td>deploy the air rifle club people to train for ...</td>\n",
              "      <td>14</td>\n",
              "      <td>[CLS] deploy the air rifle club people to trai...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>257 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   post_title  ... BERT_processed_text_length\n",
              "comment_id                                                     ...                           \n",
              "0           Husband runs off when wife is pregnant and def...  ...                         34\n",
              "1           Husband runs off when wife is pregnant and def...  ...                         53\n",
              "2           Husband runs off when wife is pregnant and def...  ...                         95\n",
              "3           Husband runs off when wife is pregnant and def...  ...                         88\n",
              "4           Husband runs off when wife is pregnant and def...  ...                         17\n",
              "...                                                       ...  ...                        ...\n",
              "252         Menacing mynas, pigeons, and crows: complaints...  ...                         11\n",
              "253         Menacing mynas, pigeons, and crows: complaints...  ...                         23\n",
              "254         Menacing mynas, pigeons, and crows: complaints...  ...                          6\n",
              "255         Menacing mynas, pigeons, and crows: complaints...  ...                         21\n",
              "256         Menacing mynas, pigeons, and crows: complaints...  ...                         14\n",
              "\n",
              "[257 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f4DcMt5ryuq",
        "colab_type": "text"
      },
      "source": [
        "### **Save processed file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQzOkv1grgFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st_df.to_csv(pwd + '/My Drive/Colab Notebooks/bert_processed_st_comments.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lx1TL2Wr3lu",
        "colab_type": "code",
        "outputId": "e75c7306-51fc-4445-ca0a-1ae3d47e1c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load model\n",
        "model = SentimentClassifierModel.load('/content/gdrive/My Drive/Colab Notebooks/' + prefix + '_model.bin', device)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentClassifierModel(\n",
              "  (bert): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZOuT0l71ZKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st_df = st_df.sort_values(by='BERT_processed_text_length', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEgmYC9C1qPo",
        "colab_type": "code",
        "outputId": "83d0354f-09c8-4ba2-eed3-dbddab24ff4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "st_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_title</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>BERT_processed_text</th>\n",
              "      <th>BERT_processed_text_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comment_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>Tsai Ing-wen re-elected Taiwan President; KMT'...</td>\n",
              "      <td>This, morning, I said , We cannot give a One P...</td>\n",
              "      <td>this , morning , i said , we cannot give a one...</td>\n",
              "      <td>221</td>\n",
              "      <td>[CLS] this , morning , i said , we cannot give...</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>Seeing so many comments on different birds inv...</td>\n",
              "      <td>seeing so many comments on different birds inv...</td>\n",
              "      <td>167</td>\n",
              "      <td>[CLS] seeing so many comments on different bir...</td>\n",
              "      <td>188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>Tsai Ing-wen re-elected Taiwan President; KMT'...</td>\n",
              "      <td>Tsai lng wen, Re-Elected,as Taiwan President, ...</td>\n",
              "      <td>tsai lng wen , re - elected , as taiwan presid...</td>\n",
              "      <td>132</td>\n",
              "      <td>[CLS] tsai lng wen , re - elected , as taiwan ...</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>They are annoying a nuisance only if they star...</td>\n",
              "      <td>they are annoying a nuisance only if they star...</td>\n",
              "      <td>128</td>\n",
              "      <td>[CLS] they are annoying a nuisance only if the...</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>Menacing mynas, pigeons, and crows: complaints...</td>\n",
              "      <td>I think many people feed the pigeons out of ki...</td>\n",
              "      <td>i think many people feed the pigeons out of ki...</td>\n",
              "      <td>124</td>\n",
              "      <td>[CLS] i think many people feed the pigeons out...</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Tsai Ing-wen re-elected Taiwan President; KMT'...</td>\n",
              "      <td>Taiwan wins</td>\n",
              "      <td>taiwan wins</td>\n",
              "      <td>2</td>\n",
              "      <td>[CLS] taiwan wins</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>Forum: Promote plant-based diet to cut Singapo...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "      <td>[CLS] yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>Forum: Promote plant-based diet to cut Singapo...</td>\n",
              "      <td>YES</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "      <td>[CLS] yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>China launches gigantic telescope in hunt for ...</td>\n",
              "      <td>Great</td>\n",
              "      <td>great</td>\n",
              "      <td>1</td>\n",
              "      <td>[CLS] great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>Forum: Promote plant-based diet to cut Singapo...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "      <td>[CLS] yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>257 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   post_title  ... BERT_processed_text_length\n",
              "comment_id                                                     ...                           \n",
              "148         Tsai Ing-wen re-elected Taiwan President; KMT'...  ...                        233\n",
              "232         Menacing mynas, pigeons, and crows: complaints...  ...                        188\n",
              "142         Tsai Ing-wen re-elected Taiwan President; KMT'...  ...                        149\n",
              "239         Menacing mynas, pigeons, and crows: complaints...  ...                        135\n",
              "249         Menacing mynas, pigeons, and crows: complaints...  ...                        131\n",
              "...                                                       ...  ...                        ...\n",
              "127         Tsai Ing-wen re-elected Taiwan President; KMT'...  ...                          2\n",
              "221         Forum: Promote plant-based diet to cut Singapo...  ...                          1\n",
              "224         Forum: Promote plant-based diet to cut Singapo...  ...                          1\n",
              "69          China launches gigantic telescope in hunt for ...  ...                          1\n",
              "219         Forum: Promote plant-based diet to cut Singapo...  ...                          1\n",
              "\n",
              "[257 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9wEMXlT1q7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cn_loss = torch.load('loss_func', map_location=lambda storage, loc: storage).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPiZ1wCW14n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ProcessedText_BERT = list(st_df.BERT_processed_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dr-hSVK1_BE",
        "colab_type": "code",
        "outputId": "89628be4-7e2b-4fc6-df4c-e16062f8cbcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ProcessedText_BERT"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this , morning , i said , we cannot give a one party , government , too much power , they will find ways to make things legally , to corrupt and abusing its power . many countries , are having a two party system . never , trusted this government , they had lost their way to govern . are singaporean , better off after so many years under this regime , the answer is definitely no . congratulations , to tsai ing - wen , successfully , re - elected as taiwan president . she , had increased , the minimum wage , in taiwan . while those past president are reluctant to do . since , our government like to changed the constitution , not my elected president and re - write the history of singapore . whereby , our beloved , ong teng chong is our first elected president . without ong teng chong , singaporean will not have the mrt system . so , this coming ge , singaporean must stand united , to re - write history once again , to vote for a majority of oppositions in parliament , to make all this ministers , that are drawing millions accountable . with all their toxic and harmful policies , that sold out singaporean .',\n",
              " \"[CLS] seeing so many comments on different birds invading the hawkers centres & housing estates makes me highlight another public nuisances -- sea otters . these animals are also invading all our water catchment areas . 10jan , 9 . 30am , canel between jurong east & jurong west , a big big family of otters , can ' t count exact # nosbut not less then a dozen of them . some are feasting on fishes caught , tilapia , while some are diving up & down hunting for fishes . further up the jogging , pcn , nearer to jurong west , we can occasionally see dropping of this animal on the pathway . what can we do if we can removed them from their invasion ? they will eventually clean up all the fishes their & create more filthy droppings on our exercise pathway . their could be a debate among which ministry department responsible ? who ' s kpi project will it be ??\",\n",
              " '[CLS] tsai lng wen , re - elected , as taiwan president , nothing to do with hong kong , dont be so naive ? taiwan had been segregated by the old generation of thinking versus the new generation . same , as singapore , the old generation of singaporean are brainwash , by pap , but not the new generation except those 69 . 1 % who are self center and selfish ? thats , the reason , why the pap government , are importing foreigners , as new citizen , 22 , 000 per year , to displace singaporean , so that this ft , so call new citizen , will be obligated , and beholden to the pap , keep voting the pap , to grip on to power .',\n",
              " '[CLS] they are annoying a nuisance only if they started visiting places where food are served like hawker centres but if they are hanging out in the estate on the ground or up the trees and do no harm other than making noises just tolerate them . in my estatethere used to be dozens of pigeons on the grass patch along the mrt track but no one care much about them including me . but if they are a nuisance like the mynah who fly into my kitchen and stole my cooked food uncovered then i will become a bird killer to protect myself in fact i killed only two in the past 20 years but if they mind their own business why should i be blood thirsty ??',\n",
              " '[CLS] i think many people feed the pigeons out of kindness . but they dont realise that : ( 1 ) the bread etc that we feed is not the nutrients that pigeons need . it actually fills them up and deprives them of searching for the right type of their natural diet . ( 2 ) the pigeons gather due to their feeding and cause complaints . this ultimately leads to tc coming to cull them . in the process other birds kena also . i think more education needs to be done so people are more aware of the implications . kindness needs to be paired with wisdom . or else they end up harming the very ones theyre trying to help .',\n",
              " \"[CLS] firstly , we should change our main source of electricity generation from burning gases to solar energy . change all cars to solar powered cars . now that will be a huge significant change . use newspapers for packaging , change to biodegradable plastics ifplastics are unavoidable . whole plant diets may upset the entire ecosystem as farmers unscrupulously clear more forests for farming land which isnt really environmental friendly to be honest ----> doesn ' t really reduce carbon footprint to be honest .\",\n",
              " '[CLS] 1 . the 99 % human herd has been corralled into various carb based diets by 1 % via various narratives . this has not benefited the 99 %. 2 . the last free range 99 % humans have made the most creative contributions by being omnivorous . [ free from the 1 %] 3 . for the herd to progress , the 99 % need to transcend the 1 % diet . there are plentiful resources on the planet , if the herd entire can recalibrate .',\n",
              " '[CLS] so sad that the marriage institution is taken so lightly these days . marriage is not easy and my husband is not perfect . he can be a lazy bum at times ! but i remind myself that i decided to marry him as much as he decided to marry me . so i will do all i can to make him happy . but of course he too needs to do all he can to make me happy . communication is key in this case . as is love , respect and understanding !',\n",
              " '[CLS] too many eateries at housing board estates ... and rubbish and food waste not properly managed . if you operate a restaurant inside the private estate , you will never see all those unwashed utensils and plates lying around feeding the birds ... the nea is responsible to ensure this kind of standards . in fact , the cost of food charged by restaurant with good standards is not higher than hdb estates . this is truth . it is all about management .',\n",
              " \"[CLS] nobody tell the truth here , most of the young people in taiwan enjoy : 1 . stagnant salary , 2 . stagnant economy improvement , 3 . what ' s the evidence ? more young people migrate out of taiwan to get better job and income ., 4 . taiwan is the back bencher of us ... and create more hoax to win in the election ., 5 . us and taiwan support directly to hk pro democracy activists .\",\n",
              " \"[CLS] may i know how much carbon footprint your suggestion is reducing . and on a wider view , is there any strong evidence that climate change is due to human behavior ? if yes , what are the biggest carbon contributor . shouldn ' t we focus on tackling the big contributing factors first ? what is the side effects and consequences plant base diet ? what we don ' t want is the government pushing something that is not useful and yet making everyone suffers .\",\n",
              " '[CLS] when a women found a men younger than her , ppl start to label . but forget ... pakcik n apek also same . like young stuff . how can u generalize ppl like that ? my ex hub was my senior but also stray away . its just human . ppl change . yes . its hurt breaking n painful . yes it will take alot of time to heal . but u can do this .',\n",
              " '[CLS] to all you commie lovers and other deluded red running dogs trying to bask in reflected glory of the wealth and power of the ccp . from berlin to budapest . from warsaw to helsinki . from urumqi to seoul . from kowloon to taipei . from athens to belgrade . when given a choice and voice , the free peoples have always said !! \" better dead than red !!\"\"',\n",
              " \"[CLS] just go to sims vista hawker centre and you know what i mean . countless complaints and photos have been sent . can our expensive and highly paid minister bang your heads together and do something - a la mrt urgency type . i know you don ' t do your feasting at hawker centres . if you do , perhaps just for pr exercise . tell me if i am wrong . sorry i have to be blunt .\",\n",
              " '[CLS] we are only 0 . 1 % of the entire population on earth . to reduce our carbon emission literally to 0 % by reverting to pre - industrial revolution and complete stone age will not change anything in the world . the best solution is a global solution or global affecting solution . if you want to reduce carbon emissions from dietary sources , you have to globally affect the 7 billion population on earth as well .',\n",
              " \"[CLS] i really do not understand women of present day , they are very capable to make 5k to 10k per months stay in condo know what the client want and needs . why can ' t they not know what a young adult want from a older women , please do not tell me is love and responsibility , come on do not fool yourself and other . just to have fun , ok .\",\n",
              " '[CLS] to reject commies influence is the right things to do god bless taiwan ! truly democratic and many are righteous and patriotic and capable their tech sector is second to none they already embark on designing and producing 3nm chip i think they are the most advance right now first piece of good news in 2020 beautiful as we move closer to chinese new year wishing taiwanese chinese a happy and fulfilling year ahead',\n",
              " \"[CLS] don ' t go overboard n fall into the trap of veganism . vegetarian diet is as old as history practise among chinese n indians over a long period . don ' t try n create artificial meat out of plants where u load it with all kinds of chemicals n sodium just to get the meat taste . don ' t try n promote artificial meat , it defeats healthy diet .\",\n",
              " '[CLS] marriage is not a investment or short term happiness experience o to experiment .. marriage has alot of commitments ... if u r not ready for that ten dnt get married ... hav safe relationship ... bcoz at the end women are the one who always becomes the pain barrier ... and the innocent child becomes the burden ...',\n",
              " '[CLS] the voters in taiwan returned president tsai , in a landslide . a defiant show against beijings pressure campaign against the island democracy , efforts to isolate her government and trying to intimidate taiwan with military , air patrols and sending carriers through the taiwan strait . the year of the pig isnt ending too well for the mao renaissance man xi .',\n",
              " '[CLS] yes , it should if the environtment is the concern . but if singapore concerns about the health of its citizens , then it should promote keto diet for its citizens by reducing the consumption of carbohydrate especially sugar , sugary drinks , and refined carbohidrate such as rice , cake , biscuits .',\n",
              " '[CLS] do not blamed the young man which is 15 years younger than her . who want a long term relationship wt a woman 15 yrs older than him !!! only for free sex & not to start a family !! he is good ! at least he did married her ! family ?? a sure no - no !!',\n",
              " \"[CLS] yes , the government should promote a plant - based diet in singapore to cut singaporeans ' carbon footprint and encourage them to eat healthily . however , it still boils to individual food preferences . also , there are conflicting studies and views on whether such diets are entirely beneficial for us . the jury is still out there .\",\n",
              " '[CLS] i submitted a video of the hdb unit feeding pigeons ... and they are still doing it . exactly like the noise nuisance hdb case ... what can u do ... on the surface talk alot , got laws ... but in reality ... nothing to stop these people',\n",
              " '[CLS] kuizhou is located in a region of forested mountains and rainfall .... not ideal for a telescope . a telescope on land should located in a sub - dessert area , where the air is dry and rare ... like those in mauna kea observatories in hawaii .',\n",
              " '[CLS] she will be delighted and continue with her corrupt team wiping off every cents cause end of the day ... the ppl will suffer but she still enjoyed her life as usual . if democracy can filled one stomach ... let it be . singapore will only gain from hongkong and taiwan uncertainty .',\n",
              " '[CLS] so ... all become vegans ? should the president , prime minister , all mps and nmps with their families and extended families start the ball rolling and sign pledge of commitment ? should the ceo down to tea lady at sph , mediacorp and their families and extended families commit the same ?',\n",
              " '[CLS] please look into crow problems at bishan as well ... these birds are evolving into norturnal animals and they crow loudly throughout the night ... it is extremely annoying cause u r trying to sleep and the noise the birds make is loud . first time seeing birds not sleeping at night .',\n",
              " '[CLS] to deter irresponsible and unfaithful man , the law should change . up to 80 % of assets and property should go to the woman . as for maintenance fees , up to at least 60 % of his earning auto deduct from his pay to his ex - wife .',\n",
              " \"[CLS] it is important to help clear your own plates so as to prevent birds from shitting and dirtying the plates . these plates will be send to central cleaning with the birds ' shits etc . the cleaners can ' t clear the plates fast enough as there are more birds than cleaners .\",\n",
              " '[CLS] every food stall sell more meat then vege . vegetarian food is so rare to get . diabetes is high but koi and so many bubble tea shop . fast food up size soft drinks . govt encourage wrong eating habbits by giving license to such stalls . how to be healthy ?',\n",
              " '[CLS] having a happy family here is not easy . this is an early stage of a journey as a family and already they are not happy . the stress will be bigger as time goes by and the national pledge is now only an aspiration . hard life for this almost perfect country .',\n",
              " '[CLS] educate the hawkers . we used to get noodles with lots of towgay . now we have to ask for it . sometimes we get scorned for asking . nowadays greens in a bowl of noodle is just symbolic : a few pieces of scallion .',\n",
              " '[CLS] what happens in hk , helped secure the election results in tw . after seeing what happened in hk , the tw people naturally will vote for non pro - china candidate . now the tw people would rather take on the hardship , but never bow down to china .',\n",
              " '[CLS] singapore could be a world leader .. ban single use plastic ( if poor african countries can then sg can ), phase out all petrol driven cars with electric , enhance family planning and encourage even smaller families , promote reduction in meat consumption , give grants for solar panels etc .',\n",
              " \"[CLS] .. kinda in this boat at the moment , feeling very regretful , turns out he didn ' t want what i wanted , it ' s a terrible thing to learn after he has has your body , i could be pregnant or worse , purely agonizing .\",\n",
              " \"[CLS] our gov ' t should introduce a law that says those above 60 years old no need to vote as they have voted enough already and with the new law can do away the merdeka and pioneer packages . gov ' t can save money .!\",\n",
              " \"[CLS] we should change our voting from 5 years to 4 years like taiwan so that our politicians don ' t have to wait for so long to try their luck to enter parliament . we voters are also tired of waiting so long to vote again .\",\n",
              " '[CLS] what do you do when you see an uncle sitting on the bench in a hdb town centre with a bag of seeds to feed a swarm of pigeons ? he is most unlikely to listen , probably far too influenced by this ...',\n",
              " '[CLS] the effect of plant based diet is beneficial only to those whose health is not so good . if one is healthy , then the effect is negligible . the choice is yours . no need to \" trouble \" the government .',\n",
              " '[CLS] hahahaha i love my meat , i eat plants ( vegetables ) when i need to get rid of constipation .. no need govt promote , i know what to do and when to eat them ..',\n",
              " '[CLS] too much talk and no courage . declare taiwan an independent state . i will salute to you . why cant have one country 2 systems . taiwan can eat the cake and enjoy their daily routine . what wrong with that .',\n",
              " '[CLS] han give people a impression that he is unique and not a normal kmt when mayor election . but he protriat himself as just another kmt who people has rejected it 4 years ago in the president election .',\n",
              " '[CLS] hmmmz . maybe the hk factor really influences the outcome ..... next step .... taiwan to be recognized as an country .... tat sure will piss fatherland china more',\n",
              " '[CLS] they stole my food from my bowl / plate in the hawker centres . putting spikes inside the hawker centre will not work as they will wait on other structures and trees outside the hawkers centres to wait for their targets .',\n",
              " \"[CLS] why she ? not nos 2 ? she didn ' t did a good job for the past 4 years , and now another lousy 4 years to go !!!!!!!!!!!!\",\n",
              " '[CLS] they chose a untrustworthy person again as their president . those who exposed her as a fraudster who used the state machinery to cover her fake phd degree and lies will continue to fight against the injustice .',\n",
              " '[CLS] the people have spoken .. eat meat eat porridge they have use their upmost intelligence to choose their leader . their future is in their own making now regardless what tsai is bringing them to .',\n",
              " '[CLS] land that grows crops for feed an entire village for a year was then used to grow animal feeding that feed cows that in turn produce meat that can only feed a family for months . food for thoughts .',\n",
              " '[CLS] there are many keyboard warriors , infringed by all this pap ibs , please , do not talk nonsense . keep to the agenda , and make us a better singapore and singaporean .',\n",
              " '[CLS] if a man flee from his responsible . he will flee in every committment . because he is not a troubleshooter but a troublemaker . and of cause he is definetly not a man',\n",
              " '[CLS] last time taiwan youths were not interested in politics . tsai should thank hk demonstrators who frighten the taiwanese youths to become out to vote for her . there are more youths than oldies in taiwan .',\n",
              " '[CLS] people tend to forget taiwan is internationally , politically isolated but not economically . the taiwanese people sees itself as chineses democratic bastion . what more can peoples republic of china do with its economic clout ?',\n",
              " '[CLS] the foundation of china chinese comminist party in 1922 for what ? ex china chinese prime minister chu said , no matter how , taiwan province election is an local authority election only ...',\n",
              " '[CLS] the husband is 15 years her junior . no wonder . what can a woman expect from a man who is 15 years younger ? the man will of course run away to find another younger woman .',\n",
              " '[CLS] forget one china policy . consider china union instead . prc would do much better if it can erase the obsession of a single china . just work towards the care and protection of the union .',\n",
              " '[CLS] someone told me , \" marriage is a gamble , u either win or u lose ...\" in my case ..... anyways , life goes on , be strong',\n",
              " '[CLS] any man with a d &$ k can be a father but it takes real balls to actually see through the marriage and take care of the kids regardless of whatever the circumstances are . period',\n",
              " '[CLS] i am very worried whenever the government \" decides \" to promote stuff concerning healthy living ... the last promotion of less sugar drinks causes the extinction of drinks like coca cola classic .',\n",
              " \"[CLS] cow turtle elephants eats plants .. do they gets slimmer hehe .. do not move faster .. slowness makes you live longer says turtle ' s hmm ...\",\n",
              " '[CLS] cue all the stupid arguments that climate change is not caused by human activity . such a tiny bubble singaporeans live in . step outside and live in the real world for once .',\n",
              " '[CLS] best feed according to pigeons is crossroads at marriot . the used plates are cleared and kept in a cubicle with curtains open . could see all pigeons having a nice buffet',\n",
              " '[CLS] yes too many birds every where temples eateries hdb void decks . i really miss sir lky governance .... he took care of our country very well .',\n",
              " '[CLS] lets not rub salt in others wound . a man chosen to wil leave even a wife given birth to half doz of children . a crow forever a crow ...',\n",
              " '[CLS] better to be happy bringing up your child than to remain with a jerk ... enjoy the process of motherhood and focus on the happy moments with the kiddo .',\n",
              " \"[CLS] start a air rifle shooting club that can shoot birds for trophy . good for sports and keep bird polulation down . i ' ll be the first one to joint\",\n",
              " '[CLS] impressive and interesting . only when you stop being politically correct and stop fighting over whose imaginary friend is the real one , then you can finally attempt to reach for the',\n",
              " '[CLS] taiwanese are so scared of china invasion to vote for her . china is not so stupid to see chinese kill chinese while the ang mos , white men laugh !',\n",
              " '[CLS] 15 years junior , mechanic and called police : shows the level immaturity ... ( no offense / pun to the job , there are responsible people )',\n",
              " '[CLS] she made her choice to go for much younger men . work both way . my ex girl ditched me when i lost my well paying job at 62 years .',\n",
              " '[CLS] eye sights must look further . today is just another episode of belting - up 2morrow n onward . i really wish taiwan all things in your favor .',\n",
              " '[CLS] this victory was given to president tsai on silver by the chinese govt . the people saw what they did in hongkong . hurray !!!',\n",
              " '[CLS] this gigantic telescope is not going to see the aliens , they are moving very fast . it still relies on refraction of light to see into space .',\n",
              " '[CLS] this gigantic telescope is not going to see the aliens , they are moving very fast . it still relies on refraction of light to see into space .',\n",
              " \"[CLS] the western world has been fooled again !! this is a chinese wok made to prepare food for the cccp ' s reunion dinner !!\",\n",
              " '[CLS] ya ... they are really annoying until i stop eating at hawker centre . i just tao bao the food to office or home to eat .',\n",
              " \"[CLS] whats with the 13 no ' s of junkhead laughter !! what hv you done for science & tech !! good job china .\",\n",
              " '[CLS] now wouldnt it be nice if they find another planet to send majority of their own to inhabit ! earths resources would then be sustainable !',\n",
              " '[CLS] choosing freedom over stability , electing democracy over economy ... strong traditional chinese values , and i am glad the taiwanese still retain them .',\n",
              " '[CLS] he is only 27 year old . he must be handsome enough for you to take a risk ! you are 42 year old . good luck !',\n",
              " \"[CLS] the chinese proverb says is not wrong - as distance may know a horse ' s strength , so time reveals a person ' s heart .\",\n",
              " '[CLS] why waste $, use the $ to educate the masses leaving your country on holiday to have some etiquette would be much appreciated .',\n",
              " '[CLS] i eat a lot of spinach wrapped in clear plastic and when i pay at sheng shiong cashier receive one more plastic bag',\n",
              " '[CLS] why would she even consider someone who is 15 years old younger than her . thinking mindset is totally sky apart .. zzz',\n",
              " \"[CLS] they ' re planning to establish a 9 dash line across the galaxy . i hope the chinese does not piss off any alien neighbor .\",\n",
              " '[CLS] authority just sit in their aircon department and shake legs . recent hip program with hanging racks has become resting areas for the birds .',\n",
              " '[CLS] never give the ccp a chance to control taiwan or hong kong . congratulationspresident tsai and the people of taiwan !',\n",
              " '[CLS] begin with push for electric cars and public transportation like buses first . shenzhen is already taking the lead and we are many years behind .',\n",
              " '[CLS] some countries are banning the use of plastic bags but not here that is more eco friendly and tackling climate change than plant based diet',\n",
              " '[CLS] china is the human herds best hope . the rest of the globe is mired in 1 %/ minion doubt and fear .',\n",
              " '[CLS] it is the latest unsustainable yuppy , fad diet that seeks to exclude nutrient dense whole food groups ...',\n",
              " '[CLS] if theres a god , there would be lives elsewhere . if not , why create so many solar systems in the universe ?',\n",
              " '[CLS] i cant blame women not having babies . some men are plain irresponsible . they are not prepared for commitments .',\n",
              " '[CLS] the aliens need to stop showering us with love .... spare some for the rest ... lol',\n",
              " '[CLS] black witch use her dirty hand to cheat people . well one day she n her team will get heavy punishment from god .',\n",
              " '[CLS] taiwan election fraud !!!!!! reporting vote , however , not recording it ???!!!',\n",
              " '[CLS] this kind of thing also need government support ? sgreans rly like govt to govern every aspect of their lives',\n",
              " \"[CLS] want to cut carbon footprint but banned pmd and tax electric car like it ' s run on diesel ...\",\n",
              " '[CLS] they gon add some szechuan peppers n stir fry the hell out of the aliens in the wok',\n",
              " \"[CLS] that ' s nice ... then extend the silk road there and after populating earth , populate the universe\",\n",
              " '[CLS] so the law allow all these able but irresponsible men to dump their child maintenance to tax payers ?',\n",
              " '[CLS] support taiwanese president tsai .... taiwan army and police become stronger .... fight against mainland china',\n",
              " '[CLS] this tension between perspective of citizens living from hand to mouth and the globalist green movement will again fracture this nation .',\n",
              " '[CLS] if vote for dpp is voting for democracy , then why need election , let dpp continue bringing down taiwan !',\n",
              " '[CLS] reduce carbon footprint is good , how about selling vegetables without packaging ? at least not in plastic bags or boxes ?',\n",
              " '[CLS] cover up your unfinished food and food scraps to deny access to the pests , with the tray if necessary',\n",
              " '[CLS] well done , taiwan , on the way to be an independent nation , the republic of taiwan , democracy & freedom',\n",
              " '[CLS] they didnt watch enough alien movies ... they might be inviting an invasion of the 3rd kind .',\n",
              " '[CLS] sling shot lah . just encourage to shoot them . tell the \" animals lover \" to do pest control',\n",
              " '[CLS] in the 70s , cats roamed the floor of hawker centers . now , the pigeons are roaming !',\n",
              " '[CLS] its a mindset to set up a family ... the joy in difficult situations values it all',\n",
              " '[CLS] all our food r imported so all have huge carbon foot print . why not recomend not eating',\n",
              " \"[CLS] i am fully committed to a plant based diet , that ' s why i take a lot of beef\",\n",
              " '[CLS] another fantastic option to support mankind in research n expedition into galactica space or 4 else relevant studies .',\n",
              " \"[CLS] what for ? don ' t we have enough trouble on earth already ? solve our earth issues first !\",\n",
              " '[CLS] wait .. this reminds me of one of the battlefield 4 map .. lol ..',\n",
              " '[CLS] oh no , taiwan back to square one again ... no breakthrough for another 4 years !',\n",
              " '[CLS] but at the end of the day , tw still belongs to china and is not independent .',\n",
              " '[CLS] best way to cut carbon footprint .... stop breathing .......',\n",
              " '[CLS] congrats taiwan ! dont let that garbage culture of chi get inside your beautiful country .',\n",
              " \"[CLS] can ' t even protect life on earth , still wanna find life in outer space ??\",\n",
              " '[CLS] no ! when our life on this earth ceases we will all be herbivores .',\n",
              " '[CLS] for several decades this prolonged birds problem still existing and worsening in a first world country .',\n",
              " '[CLS] stay strong kmt . the white sun will rise again . american propaganda is too strong .',\n",
              " '[CLS] yes please ! but promote whole plant - based fresh food instead of ultra - processed food .',\n",
              " '[CLS] when marriage is not a life - long commitment but a legal contract which you can break .',\n",
              " '[CLS] yes . is a good move . but ensure the veggie are chemical free .',\n",
              " '[CLS] every mom dream to own this gigantic wok . you know what i mean right .',\n",
              " '[CLS] both gained what they want . guy want short happiness experience . lady want long investment project',\n",
              " \"[CLS] that ' s the price we have to bear for animal lovers ... lol\",\n",
              " '[CLS] guess is really tough to be single mom . even st charges you premium for it .',\n",
              " '[CLS] think she is finding trouble by married a man who is 15 years her junior . unbelievable',\n",
              " '[CLS] crows also a danger than mynas . hope authority can do something abt it',\n",
              " \"[CLS] don ' t be fool by them . it might be a fantasy cover story .\",\n",
              " '[CLS] congratulationsto the people of taiwan ... taiwan is still taiwan ...',\n",
              " '[CLS] yes , to eat healthily and provide a more diverse veg food options !',\n",
              " '[CLS] say no to commies say no to one nation two system fk the authoritarianism',\n",
              " '[CLS] the fear of unification is far more stronger than the fear of no economic development .',\n",
              " '[CLS] be careful what u hunt for ..... it may end up hunting',\n",
              " \"[CLS] the president with power that is able to reject china ' s influence . congratulations .\",\n",
              " '[CLS] after the revamp of ava they should be more capable to tackle the problem .',\n",
              " '[CLS] the uninformed uncle and auntie contributed by indiscriminate feeding',\n",
              " '[CLS] oh wait , will the american later claim that they steal the technology from them .',\n",
              " '[CLS] well done , congratsto president tsai and to the taiwan people .',\n",
              " '[CLS] congratulationsfrom nepal ( birth place of buddha and land of mount everest )',\n",
              " '[CLS] plant based with more artificial addictive , factory making those have how many footprints',\n",
              " '[CLS] promotion of plant based diet ok but should not penalise other eaters !',\n",
              " '[CLS] ya , my friend acts like a slave to his wife . poor soul !',\n",
              " \"[CLS] they ' re so adorable though ! much better than our seagulls !\",\n",
              " '[CLS] to hunt for life beyond earth ... and then eat it .',\n",
              " '[CLS] congratsms tsai ing wen .. well done keep it up',\n",
              " \"[CLS] i ' ve read an article of this many years ago i think .\",\n",
              " '[CLS] is it they replaying the events of the novel three body problem ?',\n",
              " '[CLS] i am also in my late 40s but never wish to settle down',\n",
              " '[CLS] deploy the air rifle club people to train for olympics using real live targets',\n",
              " '[CLS] why is it all top down ? what happen to our own initiative ?',\n",
              " \"[CLS] don ' t feed them & they will go away ...!\",\n",
              " '[CLS] by raising the price of meat . oh wait . meat tax sounds good',\n",
              " '[CLS] instead of getting cheaper veg , you will get a meat tax .',\n",
              " '[CLS] the us had been looking for alien life for decades without any result',\n",
              " '[CLS] first get a telescope to look into your own hearts and minds .',\n",
              " '[CLS] grossed out to see birds eating leftovers at the hawker centers !',\n",
              " '[CLS] after win , is time to put han you yu to jail soon',\n",
              " '[CLS] hooraysay a big no to china . yes to democracy',\n",
              " '[CLS] no take care of the poor not promote for business to profit .',\n",
              " '[CLS] congratsto president tsai and the the people of taiwan .',\n",
              " \"[CLS] to be honest , the other two aren ' t got nothing !\",\n",
              " '[CLS] hunting for alien cuisine and tcm with a giant wok .',\n",
              " '[CLS] it will be start of another 4 year of suffering for taiwan !',\n",
              " '[CLS] xi jinping will have to think real hard what went wrong',\n",
              " '[CLS] stay single , no burden , no responsibility except to yourself .',\n",
              " '[CLS] whats next , promote insect based diet to save money ?',\n",
              " '[CLS] everybirdy eat vegetables , speak english ... -',\n",
              " '[CLS] congrats to president tsai and taiwanese people !!',\n",
              " '[CLS] haha what a joke after all it still belong to china',\n",
              " \"[CLS] congratulations ... she ' s got the highest mandate .\",\n",
              " '[CLS] wow can luo hey with alot of people .',\n",
              " '[CLS] yes yes ... a wise move ...',\n",
              " '[CLS] how come this photographer skill like dat one ...',\n",
              " '[CLS] no attachment . everything in life is impermanent .',\n",
              " '[CLS] a scholar cheater and liar can be re - elect',\n",
              " '[CLS] president tsai has hk to thank for her victory .',\n",
              " '[CLS] china is sensitive to the word president in tawian',\n",
              " '[CLS] their president is elected , while ours is selected ..',\n",
              " \"[CLS] with carrie lam , taiwan ' s democracy is safe !\",\n",
              " '[CLS] status quo , next few years can go tour there .',\n",
              " '[CLS] i can cut plastics and papers , but not meat .',\n",
              " '[CLS] yes we should promote the healthy plant based diet !!',\n",
              " '[CLS] can also be a giant steamboat ....',\n",
              " \"[CLS] congratulationstaiwan ' s president for winning again !\",\n",
              " '[CLS] catalyst by hk protest ... anti xi',\n",
              " '[CLS] the straits timesbecome propaganda news now',\n",
              " '[CLS] thanks to carrie lam , china lost taiwan again',\n",
              " '[CLS] a great victory for true democracy and freedom .',\n",
              " '[CLS] not surprise anymore . ww3 coming indeed',\n",
              " '[CLS] proves hkg and taiwan people hate communism .',\n",
              " '[CLS] can i launch my beyblade there ?',\n",
              " '[CLS] cat daddy used up all of his 9 lives',\n",
              " '[CLS] all our made in china products generated monry',\n",
              " '[CLS] bayblade players can have a massive battle',\n",
              " '[CLS] long live taiwan ! free hong kong now !',\n",
              " '[CLS] they just want some variety in their internment camps',\n",
              " '[CLS] we want inclusive society but only selective inclusion .',\n",
              " '[CLS] carbon dioxide is plant food ...',\n",
              " '[CLS] taiwan will have less and less friends .',\n",
              " '[CLS] politics with provoking voters at its best',\n",
              " '[CLS] can cook 1 million tons of indomie',\n",
              " '[CLS] where is the bok choy ?',\n",
              " \"[CLS] yes , it ' s a good idea\",\n",
              " '[CLS] congrats to the taiwan people',\n",
              " '[CLS] hope taiwan can be good again .',\n",
              " '[CLS] after the piaking comes the suffering',\n",
              " '[CLS] the hk episode help her to win',\n",
              " '[CLS] we are not goats !!!',\n",
              " '[CLS] maybe its not true love after all',\n",
              " '[CLS] as if i will subscribe .',\n",
              " '[CLS] congratulationspresident tsai !',\n",
              " '[CLS] down the road for the country .',\n",
              " '[CLS] congratulationspresident tsai !',\n",
              " '[CLS] congratulations as no one needs another hk',\n",
              " '[CLS] the end of taiwan ...',\n",
              " '[CLS] one belt one road one universe',\n",
              " '[CLS] this is human advance ment',\n",
              " '[CLS] what there will find is b',\n",
              " '[CLS] what kind of insanity is this',\n",
              " '[CLS] send in the eagles !!',\n",
              " '[CLS] finding for new things to eat',\n",
              " '[CLS] vegetable is better than fish .',\n",
              " '[CLS] no life beyong earth',\n",
              " '[CLS] mostly to find new food',\n",
              " '[CLS] nice beyblade stadium',\n",
              " '[CLS] congratulations to president tsai',\n",
              " '[CLS] not to monitor us ?',\n",
              " '[CLS] kmt deserves it !',\n",
              " '[CLS] china will not be pleased',\n",
              " '[CLS] botak .. lose',\n",
              " '[CLS] battlefield 4 vibes',\n",
              " '[CLS] into the unknown .',\n",
              " '[CLS] made in china .',\n",
              " '[CLS] 15 years her junior',\n",
              " '[CLS] not a surprise .',\n",
              " '[CLS] yes !!!',\n",
              " '[CLS] china owns the galaxy',\n",
              " '[CLS] absolutely agree with this',\n",
              " '[CLS] taiwanese got guts',\n",
              " '[CLS] salute to china',\n",
              " '[CLS] very impressive',\n",
              " '[CLS] death star',\n",
              " '[CLS] yes please',\n",
              " '[CLS] great job',\n",
              " '[CLS] been there',\n",
              " '[CLS] congratulations taiwan',\n",
              " '[CLS] taiwan wins',\n",
              " '[CLS] yes',\n",
              " '[CLS] yes',\n",
              " '[CLS] great',\n",
              " '[CLS] yes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlCsN4Rc2AGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax = torch.nn.Softmax(dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzmO2Are7vP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68m9oGBiBE9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e0a0c6c1-52d7-4e72-d7f4-41552fc7ab3d"
      },
      "source": [
        "sents = ProcessedText_BERT[:2]\n",
        "sents"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this , morning , i said , we cannot give a one party , government , too much power , they will find ways to make things legally , to corrupt and abusing its power . many countries , are having a two party system . never , trusted this government , they had lost their way to govern . are singaporean , better off after so many years under this regime , the answer is definitely no . congratulations , to tsai ing - wen , successfully , re - elected as taiwan president . she , had increased , the minimum wage , in taiwan . while those past president are reluctant to do . since , our government like to changed the constitution , not my elected president and re - write the history of singapore . whereby , our beloved , ong teng chong is our first elected president . without ong teng chong , singaporean will not have the mrt system . so , this coming ge , singaporean must stand united , to re - write history once again , to vote for a majority of oppositions in parliament , to make all this ministers , that are drawing millions accountable . with all their toxic and harmful policies , that sold out singaporean .',\n",
              " \"[CLS] seeing so many comments on different birds invading the hawkers centres & housing estates makes me highlight another public nuisances -- sea otters . these animals are also invading all our water catchment areas . 10jan , 9 . 30am , canel between jurong east & jurong west , a big big family of otters , can ' t count exact # nosbut not less then a dozen of them . some are feasting on fishes caught , tilapia , while some are diving up & down hunting for fishes . further up the jogging , pcn , nearer to jurong west , we can occasionally see dropping of this animal on the pathway . what can we do if we can removed them from their invasion ? they will eventually clean up all the fishes their & create more filthy droppings on our exercise pathway . their could be a debate among which ministry department responsible ? who ' s kpi project will it be ??\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcGOnpxdBMJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e87ab21f-007b-401e-df8e-da614ab2b265"
      },
      "source": [
        "len(sents)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13KzlM5rBI0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "489c9625-5bdf-45d8-8d61-f5c430ee97cf"
      },
      "source": [
        "pre_softmax = model(sents)[0]\n",
        "pre_softmax"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.6207, -1.1077, -1.3669],\n",
              "        [ 2.4422, -0.3667, -1.6395]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRquPU8bBf-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a0c1cd9-fa7b-4b6b-85c8-327e60e84268"
      },
      "source": [
        "pre_softmax.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsOHI8GFBO_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18b99229-0e6a-422e-e471-8812ecaf71ba"
      },
      "source": [
        "prob = softmax(pre_softmax)\n",
        "prob"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9592, 0.0230, 0.0178],\n",
              "        [0.9284, 0.0560, 0.0157]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdINBYKjBhlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f874aace-83e7-41d9-aea2-0a94476cdeab"
      },
      "source": [
        "prob.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeHeWtVGB8Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cbd6979-e782-4a65-fd9c-b6b6dfbf18c7"
      },
      "source": [
        "prob[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9592, 0.0230, 0.0178], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVkRsEenBqL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the highest value of the tensor\n",
        "label_indexes = [t.item() for t in list(torch.argmax(prob, dim=1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZcarIzrCAAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fb0efc9-283a-4e28-f387-392c5becc76e"
      },
      "source": [
        "prediction = labels[label_indexes[1]]\n",
        "prediction"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcX7vgso2Ia6",
        "colab_type": "code",
        "outputId": "f2bcc15b-1682-464c-f966-df0a31723435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predictions = []\n",
        "with torch.no_grad():\n",
        "  sents = ProcessedText_BERT\n",
        "  pre_softmax = model(sents)[0]\n",
        "  prob = softmax(pre_softmax)\n",
        "  predictions.extend([t.item() for t in list(torch.argmax(prob, dim=1))])\n",
        "print(predictions)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 1, 2, 0, 2, 2, 2, 1, 0, 0, 2, 0, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 0, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUlRgUYBB_PH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74981608-6359-4850-8231-1efa9417bfc1"
      },
      "source": [
        "[labels[pred_val] for pred_val in predictions]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'positive',\n",
              " 'neutral']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Zo-sBu2zXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st_df['predictions'] = [labels[pred_val] for pred_val in predictions]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubeg6xH14ofS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f409b35d-4b99-4000-fd69-8dbf02148997"
      },
      "source": [
        "print(st_df.comment_text[90])\n",
        "print(st_df.predictions[90])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done,Â congratsÂ to President Tsai and to the Taiwan people.\n",
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kufYeGGlFlAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st_df.to_csv(pwd + '/My Drive/Colab Notebooks/bert_predicted_st_comments.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8_Plf-gGnwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "28789a8a-ab7d-43ca-e55c-f154b4b34095"
      },
      "source": [
        "st_df.predictions.value_counts()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    136\n",
              "neutral      67\n",
              "positive     54\n",
              "Name: predictions, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkTFADxUGwhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}